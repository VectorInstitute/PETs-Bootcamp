{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "reported-membership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcd0ecaf590>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Restrict minor warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to display all outputs of one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer as CTT\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import syft as sy\n",
    "from uuid import UUID\n",
    "from uuid import uuid4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "quality-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.psi.util import Client, Server\n",
    "from src.utils import add_ids\n",
    "from src.utils.data_utils import id_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-burton",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fifth-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"/ssd003/projects/pets/datasets/caravan-insurance-challenge.csv\"\n",
    "df1 = pd.read_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "charitable-tourism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9822, 87)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "shared-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['MOSTYPE','MOSHOOFD']\n",
    "df = pd.get_dummies(df1, columns = categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "religious-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['ORIGIN']=='train']\n",
    "val = df[df['ORIGIN']=='test']\n",
    "\n",
    "_ = train.pop('ORIGIN')\n",
    "_ = val.pop('ORIGIN')\n",
    "\n",
    "X_train = train\n",
    "X_val = val\n",
    "y_train = train.pop('CARAVAN')\n",
    "y_val = val.pop('CARAVAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "hired-beijing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5822, 133), (5822,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "local-advertising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 133), (4000,))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "noticed-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "polar-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ?\n",
    "scaler = StandardScaler() # change?\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "registered-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "asian-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset(X_train, y_train)\n",
    "dataset_val = dataset(X_val, y_val)\n",
    "train_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "smaller-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "# batch_size to tune\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=512, shuffle=False)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-sussex",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "collect-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntactModel(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    Model for the Intact dataset\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dim: \n",
    "        Dimensionality of Intact Data\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Intact Model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim): \n",
    "        super(IntactModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        pred = self.layers(feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-blake",
   "metadata": {},
   "source": [
    "# Initialize and Configure Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "after-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_weights(class1_size, class2_size):\n",
    "    if class1_size < class2_size:\n",
    "        return [class2_size / class1_size, 1]\n",
    "        factor2 = 1\n",
    "    else:\n",
    "        return [1, class1_size / class2_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "african-blanket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device to train on\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "final-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 9236\n",
      "[15.761092150170649, 1]\n"
     ]
    }
   ],
   "source": [
    "# check relative weights of classes\n",
    "class1_size = df[df['CARAVAN'] == 1].shape[0]\n",
    "class0_size = df[df['CARAVAN'] == 0].shape[0]\n",
    "print(class1_size, class0_size)\n",
    "weights = get_classes_weights(class1_size, class0_size)\n",
    "print(weights)\n",
    "weights = torch.tensor(weights[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "desirable-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IntactModel(train_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001,  betas=(0.9, 0.999))\n",
    "criterion = nn.BCELoss(weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-digit",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "gross-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataloader, model, optmizer, criterion):\n",
    "    running_loss = 0\n",
    "    for X, y in dataloader:\n",
    "       \n",
    "#        print(y.shape)\n",
    "#        print(y.unsqueeze(1))\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Zero our grads\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Make a prediction\n",
    "        pred = model(X)\n",
    "\n",
    "        # Figure out how much we missed by\n",
    "        loss = criterion(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backprop the loss on the end layer\n",
    "        loss.backward()\n",
    "    \n",
    "        # Change the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate Loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "athletic-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(dataloader, model, optimizer, criterion):\n",
    "    running_loss = 0\n",
    "    exs = 0 \n",
    "    correct = 0\n",
    "    aucs = []\n",
    "    f1s = []\n",
    "    for (X, y) in dataloader:\n",
    "        # Send data and labels to machine model is on\n",
    "        y  = y.to(device)\n",
    "        X = X.to(device)     \n",
    "   \n",
    "        # Make a prediction\n",
    "        with torch.no_grad():\n",
    "            pred = model.forward(X).squeeze()\n",
    "        \n",
    "        #Calcualte Loss\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        # Put back on cpu\n",
    "        pred = pred.cpu().float()\n",
    "        y = y.cpu().int()\n",
    "\n",
    "        #Calculate AUC\n",
    "        thresh_pred = (pred > .5).float()\n",
    "        thresh_pred = thresh_pred.int()\n",
    "\n",
    "        # Fix Me: Undefined for batches with all-same labels...\n",
    "        auc = roc_auc_score(y, pred)\n",
    "        f1 = f1_score(y, thresh_pred)\n",
    "\n",
    "        # Calculate Accuracy Components\n",
    "        num_exs = X.shape[0]\n",
    "        num_correct = torch.sum(thresh_pred == y).item()\n",
    "\n",
    "        # Accumulate loss, accuracy and auc\n",
    "        exs += num_exs\n",
    "        correct += num_correct\n",
    "        running_loss += loss.item()\n",
    "        aucs.append(auc)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    auc = np.mean(np.array(aucs))\n",
    "    f1 = np.mean(np.array(f1s))\n",
    "    accuracy = correct / exs\n",
    "\n",
    "    return f1, accuracy, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "electronic-february",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntactModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "IntactModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t F1: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IntactModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "IntactModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t F1: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IntactModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "IntactModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \t F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "metric_names = [\"Train Loss\", \"Validation Loss\", \"Accuracy\", \"F1\"]\n",
    "metrics = {metric:[] for metric in metric_names}\n",
    "epochs = 3\n",
    "\n",
    "# Train Loop\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Train Step\n",
    "    model.train()\n",
    "    train_loss = train_step(dataloader_train, model, optimizer, criterion)\n",
    "\n",
    "    # Train Step\n",
    "    model.eval()\n",
    "    f1, accuracy, val_loss = val_step(dataloader_val, model, optimizer, criterion)\n",
    "    \n",
    "    # Log metrics\n",
    "    print(f\"Epoch: {i} \\t F1: {f1}\")\n",
    "    metrics[\"Train Loss\"].append(train_loss)\n",
    "    metrics[\"Validation Loss\"].append(val_loss)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"F1\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-experiment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pets_ALL",
   "language": "python",
   "name": "pets_all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
