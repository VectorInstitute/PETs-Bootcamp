{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reported-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Restrict minor warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to display all outputs of one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer as CTT\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import syft as sy\n",
    "from uuid import UUID\n",
    "from uuid import uuid4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expanded-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quality-phrase",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3262/2045112964.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mid_collate_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.psi.util import Client, Server\n",
    "from src.utils import add_ids\n",
    "from src.utils.data_utils import id_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "designed-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_weights(class1_size, class2_size):\n",
    "    if class1_size < class2_size:\n",
    "        return [class2_size / class1_size, 1]\n",
    "        factor2 = 1\n",
    "    else:\n",
    "        return [1, class1_size / class2_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "collectible-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalDataset(Dataset):\n",
    "    \"\"\"Dataset for Vertical Federated Learning\"\"\"\n",
    "\n",
    "    def __init__(self, ids, data, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ids (Numpy Array) : Numpy Array with UUIDS\n",
    "            data (Numpy Array) : Numpy Array with Features\n",
    "            targets (Numpy Array) : Numpy Array with Labels. None if not available. \n",
    "        \"\"\"\n",
    "        self.ids = ids\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return record single record\"\"\"\n",
    "        feature = self.data[index].astype(np.float32)\n",
    "\n",
    "        if self.labels is None:\n",
    "            label  = None\n",
    "        else:\n",
    "            label = int(self.labels[index]) if self.labels is not None else None\n",
    "\n",
    "        id = self.ids[index]\n",
    "\n",
    "        # Return a tuple of non-None elements\n",
    "        return (*filter(lambda x: x is not None, (feature, label, id)),)\n",
    "    \n",
    "    def get_ids(self):\n",
    "        \"\"\"Return a list of the ids of this dataset.\"\"\"\n",
    "        return [str(id_) for id_ in self.ids]\n",
    "    \n",
    "    def sort_by_ids(self):\n",
    "        \"\"\"\n",
    "        Sort the dataset by IDs in ascending order\n",
    "        \"\"\"\n",
    "        ids = self.get_ids()\n",
    "        sorted_idxs = np.argsort(ids)\n",
    "\n",
    "\n",
    "        self.data = self.data[sorted_idxs]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            self.labels = self.labels[sorted_idxs]\n",
    "\n",
    "        self.ids = self.ids[sorted_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "valuable-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePartitionDataLoader(DataLoader):\n",
    "    \"\"\"DataLoader for a single vertically-partitioned dataset\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.collate_fn = id_collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "appreciated-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalDataLoader:\n",
    "    \"\"\"Dataloader which batches data from a complete\n",
    "    set of vertically-partitioned datasets\n",
    "    i.e. the images dataset AND the labels dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data1, data2, *args, **kwargs):\n",
    "\n",
    "        self.dataloader1 = SinglePartitionDataLoader(\n",
    "            data1, *args, **kwargs\n",
    "        )\n",
    "        self.dataloader2 = SinglePartitionDataLoader(\n",
    "            data2, *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Zip Dataloaders \n",
    "        \"\"\"\n",
    "        return zip(self.dataloader1, self.dataloader2)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return length of dataset\n",
    "        \"\"\"\n",
    "        return (len(self.dataloader1) + len(self.dataloader2)) // 2\n",
    "\n",
    "    def drop_non_intersecting(self, intersection):\n",
    "        \"\"\"Remove elements and ids in the datasets that are not in the intersection.\"\"\"\n",
    "        self.dataloader1.dataset.data = self.dataloader1.dataset.data[intersection]\n",
    "        self.dataloader1.dataset.ids = self.dataloader1.dataset.ids[intersection]\n",
    "\n",
    "        self.dataloader1.dataset.labels = self.dataloader1.dataset.labels[intersection]\n",
    "        self.dataloader2.dataset.ids = self.dataloader2.dataset.ids[intersection]\n",
    "\n",
    "    def sort_by_ids(self) -> None:\n",
    "        \"\"\"\n",
    "        Sort each dataset by ids\n",
    "        \"\"\"\n",
    "        self.dataloader1.dataset.sort_by_ids()\n",
    "        self.dataloader2.dataset.sort_by_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "superb-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaravanModel(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    Model for the caravan dataset\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dim: \n",
    "        Dimensionality of Caravan Data (in-house + vendor)\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Caravan Model\n",
    "    \"\"\"\n",
    "    def __init__(self, house_dim, vendor_dim): \n",
    "        super(CaravanModel, self).__init__()\n",
    "        self.fused_input_dim = house_dim + vendor_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.fused_input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, house_feat, vendor_feat):\n",
    "        feat = torch.cat([house_feat, vendor_feat], dim=1)\n",
    "        pred = self.layers(feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "existing-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VendorModel(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    Model for Vendor variables\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dim: \n",
    "        Dimensionality of the vendor data\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Credit Bureau Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cb_dim): \n",
    "        super(VendorModel, self).__init__()\n",
    "        self.cb_dim = cb_dim\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Linear(self.cb_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, cb_feat):\n",
    "        pred = self.layers(cb_feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "enclosed-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    \"\"\"\n",
    "    A class representing SplitNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    caravan_model:  \n",
    "        Model for Caravan Neural Network Module\n",
    "\n",
    "    vendor_model:   \n",
    "        Vendor Neural Network Module\n",
    "\n",
    "    caravan_opt:  \n",
    "        Optimizer for the Caravan Neural Network Module\n",
    "\n",
    "    vendor_opt:   \n",
    "        Optimizer for the Vendor Neural Network Module\n",
    "\n",
    "    data: \n",
    "        A list storing intermediate computations at each index\n",
    "\n",
    "    remote_tensors: \n",
    "        A list storing intermediate computations at each index (Computation from each model detached from global computation graph)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the SplitNN\n",
    "\n",
    "    backward(): \n",
    "        Performs a backward pass through the SplitNN\n",
    "\n",
    "    zero_grads():\n",
    "        Zeros the gradients of all networks in SplitNN\n",
    "\n",
    "    step():\n",
    "        Updates the parameters of all networks in SplitNN\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, caravan_model, vendor_model, caravan_opt, vendor_opt):\n",
    "        self.caravan_model = caravan_model\n",
    "        self.vendor_model = vendor_model\n",
    "        self.caravan_opt = caravan_opt\n",
    "        self.vendor_opt = vendor_opt\n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "\n",
    "    def forward(self, hc_x, cb_x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:  \n",
    "            Input Sample \n",
    "        \"\"\"\n",
    "\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "        # Forward pass through first model\n",
    "        data.append(self.cb_model(cb_x))\n",
    "\n",
    "        # if location of data is the same as location of the subsequent model\n",
    "        if data[-1].location == self.hc_model.location:\n",
    "            # store computation in remote tensor array \n",
    "            # Gradients will be only computed backward upto the point of detachment\n",
    "            remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "        else:\n",
    "            # else move data to location of subsequent model and store computation in remote tensor array \n",
    "            # Gradients will be only computed backward upto the point of detachment\n",
    "            remote_tensors.append(\n",
    "                data[-1].detach().move(self.hc_model.location).requires_grad_()\n",
    "            )\n",
    "\n",
    "        # Get and return final output of model\n",
    "        data.append(self.hc_model(hc_x, remote_tensors[-1]))\n",
    "\n",
    "        self.data = data \n",
    "        self.remote_tensors = remote_tensors\n",
    "        return data[-1]\n",
    "\n",
    "    def backward(self):\n",
    "        # if location of data is the same as detatched data \n",
    "        if self.remote_tensors[0].location == self.data[0].location:\n",
    "            # Store gradients from remote_tensor \n",
    "            grads = self.remote_tensors[0].grad.copy()\n",
    "        else:\n",
    "            # Move gradients to lovation of Store grad\n",
    "            grads = self.remote_tensors[0].grad.copy().move(self.data[0].location)\n",
    "\n",
    "        self.data[0].backward(grads)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.cb_opt.zero_grad()\n",
    "        self.hc_opt.zero_grad()\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.cb_opt.step()\n",
    "        self.hc_opt.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-burton",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fifth-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"/ssd003/projects/pets/datasets/caravan-insurance-challenge.csv\"\n",
    "df = pd.read_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "charitable-tourism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9822, 87)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "religious-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['ORIGIN']=='train']\n",
    "val = df[df['ORIGIN']=='test']\n",
    "\n",
    "_ = train.pop('ORIGIN')\n",
    "_ = val.pop('ORIGIN')\n",
    "\n",
    "X_train = train\n",
    "X_val = val\n",
    "y_train = train.pop('CARAVAN')\n",
    "y_val = val.pop('CARAVAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "needed-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numerical columns\n",
    "cat_cols = ['MOSTYPE','MOSHOOFD']\n",
    "num_cols = list(X_train.columns.values[43:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "latest-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to numpy arrays\n",
    "# create 2 datasets: one categorial, one numerical\n",
    "X_train_num = np.array(X_train[num_cols])\n",
    "X_train_cat = np.array(X_train[cat_cols])\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val_num = np.array(X_val[cat_cols])\n",
    "X_val_cat = np.array(X_val[cat_cols])\n",
    "y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hired-beijing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5822, 42), (5822, 2), (5822,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_num.shape, X_train_cat.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "local-advertising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 2), (4000, 2), (4000,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_num.shape, X_val_cat.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sound-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UID Column\n",
    "uuids = np.array([uuid4() for _ in range(len(X_train))])\n",
    "uuids_val = np.array([uuid4() for _ in range(len(X_val))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "strong-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VerticalDatasets\n",
    "num_dataset = VerticalDataset(ids=uuids, data=X_train_num, labels=y_train)\n",
    "cat_dataset = VerticalDataset(ids=uuids, data=X_train_cat, labels=y_train)\n",
    "\n",
    "num_dataset_val = VerticalDataset(ids=uuids_val, data=X_val_num, labels=y_val)\n",
    "cat_dataset_val = VerticalDataset(ids=uuids_val, data=X_val_cat, labels=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fancy-manual",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id_collate_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3262/2604309076.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Initialize Train Dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVerticalDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute private set intersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclient_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3262/1600622780.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data1, data2, *args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         self.dataloader1 = SinglePartitionDataLoader(\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         )\n",
      "\u001b[0;32m/tmp/ipykernel_3262/1760888303.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_collate_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'id_collate_fn' is not defined"
     ]
    }
   ],
   "source": [
    "## Initialize Train Dataloader \n",
    "dataloader = VerticalDataLoader(num_dataset, cat_dataset, batch_size=512)\n",
    "\n",
    "# Compute private set intersection\n",
    "client_items = dataloader.dataloader1.dataset.get_ids()\n",
    "server_items = dataloader.dataloader2.dataset.get_ids()\n",
    " \n",
    "client = Client(client_items)\n",
    "server = Server(server_items)\n",
    "\n",
    "setup, response = server.process_request(client.request, len(client_items))\n",
    "intersection = client.compute_intersection(setup, response)\n",
    "\n",
    "# Order data\n",
    "dataloader.drop_non_intersecting(intersection)\n",
    "dataloader.sort_by_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "guilty-contamination",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id_collate_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3262/3094026547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Initialize Validation Dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVerticalDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_dataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute private set intersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_client_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3262/1600622780.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data1, data2, *args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         self.dataloader1 = SinglePartitionDataLoader(\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         )\n",
      "\u001b[0;32m/tmp/ipykernel_3262/1760888303.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_collate_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'id_collate_fn' is not defined"
     ]
    }
   ],
   "source": [
    "## Initialize Validation Dataloader \n",
    "val_dataloader = VerticalDataLoader(num_dataset_val, cat_dataset_val, batch_size=512)\n",
    "\n",
    "# Compute private set intersection\n",
    "val_client_items = val_dataloader.dataloader1.dataset.get_ids()\n",
    "val_server_items = val_dataloader.dataloader2.dataset.get_ids()\n",
    "\n",
    "val_client = Client(val_client_items)\n",
    "val_server = Server(val_server_items)\n",
    "\n",
    "val_setup, val_response = val_server.process_request(val_client.request, len(val_client_items))\n",
    "val_intersection = val_client.compute_intersection(val_setup, val_response)\n",
    "\n",
    "# Order data\n",
    "val_dataloader.drop_non_intersecting(val_intersection)\n",
    "val_dataloader.sort_by_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-sussex",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "soviet-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device to train on\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "specialized-assets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 9236\n",
      "[15.761092150170649, 1]\n"
     ]
    }
   ],
   "source": [
    "# check relative weights of classes\n",
    "class1_size = df[df['CARAVAN'] == 1].shape[0]\n",
    "class0_size = df[df['CARAVAN'] == 0].shape[0]\n",
    "print(class1_size, class0_size)\n",
    "weights = get_classes_weights(class1_size, class0_size)\n",
    "print(weights)\n",
    "weights = torch.tensor(weights).to(device)\n",
    "#criterion = nn.?(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "varying-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training globals \n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Iniatialize Caravan numerical Model and Optimizer\n",
    "caravan_model = CaravanModel(len(num_cols), len(cat_cols))\n",
    "caravan_opt = torch.optim.Adam(caravan_model.parameters(), lr=.001,  betas=(0.9, 0.999))\n",
    "\n",
    "# Iniatialize Credit Bureau Model and Optmizer\n",
    "vendor_model = VendorModel(len(cat_cols))\n",
    "vendor_opt = torch.optim.Adam(vendor_model.parameters(), lr=.001,  betas=(0.9, 0.999))\n",
    "\n",
    "# Define Split Neural Network\n",
    "splitNN = SplitNN(caravan_model, vendor_model, caravan_opt, vendor_opt)\n",
    "criterion = torch.nn.BCELoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pets_ALL",
   "language": "python",
   "name": "pets_all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
