{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reported-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "import syft as sy\n",
    "from uuid import UUID\n",
    "from uuid import uuid4\n",
    "\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quality-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.psi.util import Client, Server\n",
    "from src.utils import add_ids\n",
    "from src.utils.data_utils import id_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collectible-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalDataset(Dataset):\n",
    "    \"\"\"Dataset for Vertical Federated Learning\"\"\"\n",
    "\n",
    "    def __init__(self, ids, data, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ids (Numpy Array) : Numpy Array with UUIDS\n",
    "            data (Numpy Array) : Numpy Array with Features\n",
    "            targets (Numpy Array) : Numpy Array with Labels. None if not available. \n",
    "        \"\"\"\n",
    "        self.ids = ids\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return record single record\"\"\"\n",
    "        feature = self.data[index].astype(np.float32)\n",
    "\n",
    "        if self.labels is None:\n",
    "            label  = None\n",
    "        else:\n",
    "            label = int(self.labels[index]) if self.labels is not None else None\n",
    "\n",
    "        id = self.ids[index]\n",
    "\n",
    "        # Return a tuple of non-None elements\n",
    "        return (*filter(lambda x: x is not None, (feature, label, id)),)\n",
    "    \n",
    "    def get_ids(self):\n",
    "        \"\"\"Return a list of the ids of this dataset.\"\"\"\n",
    "        return [str(id_) for id_ in self.ids]\n",
    "    \n",
    "    def sort_by_ids(self):\n",
    "        \"\"\"\n",
    "        Sort the dataset by IDs in ascending order\n",
    "        \"\"\"\n",
    "        ids = self.get_ids()\n",
    "        sorted_idxs = np.argsort(ids)\n",
    "\n",
    "\n",
    "        self.data = self.data[sorted_idxs]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            self.labels = self.labels[sorted_idxs]\n",
    "\n",
    "        self.ids = self.ids[sorted_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-snowboard",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "french-swiss",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PWAPART</th>\n",
       "      <th>PWABEDR</th>\n",
       "      <th>PWALAND</th>\n",
       "      <th>PPERSAUT</th>\n",
       "      <th>PBESAUT</th>\n",
       "      <th>PMOTSCO</th>\n",
       "      <th>PVRAAUT</th>\n",
       "      <th>PAANHANG</th>\n",
       "      <th>PTRACTOR</th>\n",
       "      <th>PWERKT</th>\n",
       "      <th>...</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "      <th>UUID</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>CARAVAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cb8db3d9-e0c2-4f2c-a866-d579a1d61ff4</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>eed33082-7621-473d-b255-c5d35198627a</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7950c7c4-4003-44d0-88dc-b05e6437afd4</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6b807aa1-88d8-4ab0-ad1b-a8ef051f91e1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>c1100358-d07d-4d70-aad0-49ace40eadda</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PWAPART  PWABEDR  PWALAND  PPERSAUT  PBESAUT  PMOTSCO  PVRAAUT  PAANHANG  \\\n",
       "0  0.000000      0.0      0.0  0.666667      0.0      0.0      0.0       0.0   \n",
       "1  0.666667      0.0      0.0  0.000000      0.0      0.0      0.0       0.0   \n",
       "2  0.666667      0.0      0.0  0.666667      0.0      0.0      0.0       0.0   \n",
       "3  0.000000      0.0      0.0  0.666667      0.0      0.0      0.0       0.0   \n",
       "4  0.000000      0.0      0.0  0.000000      0.0      0.0      0.0       0.0   \n",
       "\n",
       "   PTRACTOR  PWERKT  ...  AWAOREG    ABRAND  AZEILPL  APLEZIER  AFIETS  \\\n",
       "0       0.0     0.0  ...      0.0  0.142857      0.0       0.0     0.0   \n",
       "1       0.0     0.0  ...      0.0  0.142857      0.0       0.0     0.0   \n",
       "2       0.0     0.0  ...      0.0  0.142857      0.0       0.0     0.0   \n",
       "3       0.0     0.0  ...      0.0  0.142857      0.0       0.0     0.0   \n",
       "4       0.0     0.0  ...      0.0  0.142857      0.0       0.0     0.0   \n",
       "\n",
       "   AINBOED  ABYSTAND                                  UUID  ORIGIN  CARAVAN  \n",
       "0      0.0       0.0  cb8db3d9-e0c2-4f2c-a866-d579a1d61ff4   train        0  \n",
       "1      0.0       0.0  eed33082-7621-473d-b255-c5d35198627a   train        0  \n",
       "2      0.0       0.0  7950c7c4-4003-44d0-88dc-b05e6437afd4   train        0  \n",
       "3      0.0       0.0  6b807aa1-88d8-4ab0-ad1b-a8ef051f91e1   train        0  \n",
       "4      0.0       0.0  c1100358-d07d-4d70-aad0-49ace40eadda   train        0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Intact Data\n",
    "data_dir = \"/ssd003/projects/pets/datasets\"\n",
    "INTACT_DATA_PATH = f\"{data_dir}/prdct_insurance_stats_info.csv\"\n",
    "intact_df_full = pd.read_csv(INTACT_DATA_PATH)\n",
    "intact_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satisfactory-constant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>MRELSA</th>\n",
       "      <th>MRELOV</th>\n",
       "      <th>...</th>\n",
       "      <th>MOSHOOFD_3</th>\n",
       "      <th>MOSHOOFD_4</th>\n",
       "      <th>MOSHOOFD_5</th>\n",
       "      <th>MOSHOOFD_6</th>\n",
       "      <th>MOSHOOFD_7</th>\n",
       "      <th>MOSHOOFD_8</th>\n",
       "      <th>MOSHOOFD_9</th>\n",
       "      <th>MOSHOOFD_10</th>\n",
       "      <th>UUID</th>\n",
       "      <th>ORIGIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cb8db3d9-e0c2-4f2c-a866-d579a1d61ff4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eed33082-7621-473d-b255-c5d35198627a</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7950c7c4-4003-44d0-88dc-b05e6437afd4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6b807aa1-88d8-4ab0-ad1b-a8ef051f91e1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>c1100358-d07d-4d70-aad0-49ace40eadda</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MAANTHUI  MGEMOMV  MGEMLEEF    MGODRK    MGODPR  MGODOV    MGODGE  \\\n",
       "0       0.0      0.4       0.2  0.000000  0.555556     0.2  0.333333   \n",
       "1       0.0      0.2       0.2  0.111111  0.444444     0.2  0.444444   \n",
       "2       0.0      0.2       0.2  0.000000  0.444444     0.4  0.444444   \n",
       "3       0.0      0.4       0.4  0.222222  0.333333     0.4  0.444444   \n",
       "4       0.0      0.6       0.2  0.111111  0.444444     0.2  0.444444   \n",
       "\n",
       "     MRELGE    MRELSA    MRELOV  ...  MOSHOOFD_3  MOSHOOFD_4  MOSHOOFD_5  \\\n",
       "0  0.777778  0.000000  0.222222  ...           0           0           0   \n",
       "1  0.666667  0.285714  0.222222  ...           0           0           0   \n",
       "2  0.333333  0.285714  0.444444  ...           0           0           0   \n",
       "3  0.555556  0.285714  0.222222  ...           1           0           0   \n",
       "4  0.777778  0.142857  0.222222  ...           0           0           0   \n",
       "\n",
       "   MOSHOOFD_6  MOSHOOFD_7  MOSHOOFD_8  MOSHOOFD_9  MOSHOOFD_10  \\\n",
       "0           0           0           1           0            0   \n",
       "1           0           0           1           0            0   \n",
       "2           0           0           1           0            0   \n",
       "3           0           0           0           0            0   \n",
       "4           0           0           0           0            1   \n",
       "\n",
       "                                   UUID  ORIGIN  \n",
       "0  cb8db3d9-e0c2-4f2c-a866-d579a1d61ff4   train  \n",
       "1  eed33082-7621-473d-b255-c5d35198627a   train  \n",
       "2  7950c7c4-4003-44d0-88dc-b05e6437afd4   train  \n",
       "3  6b807aa1-88d8-4ab0-ad1b-a8ef051f91e1   train  \n",
       "4  c1100358-d07d-4d70-aad0-49ace40eadda   train  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Vendor Data\n",
    "VENDOR_DATA_PATH = f\"{data_dir}/demograhic_info.csv\"\n",
    "vendor_df_full = pd.read_csv(VENDOR_DATA_PATH)\n",
    "vendor_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expressed-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carve out validation set\n",
    "assert intact_df_full.shape[0] == vendor_df_full.shape[0]\n",
    "\n",
    "intact_df = intact_df_full[intact_df_full['ORIGIN'] == 'train']\n",
    "intact_df_val = intact_df_full[intact_df_full['ORIGIN'] == 'test']\n",
    "\n",
    "_ = intact_df.pop('ORIGIN')\n",
    "_ = intact_df_val.pop('ORIGIN')\n",
    "\n",
    "vendor_df = vendor_df_full[vendor_df_full['ORIGIN'] =='train']\n",
    "vendor_df_val = vendor_df_full[vendor_df_full['ORIGIN']=='test']\n",
    "\n",
    "_ = vendor_df.pop('ORIGIN')\n",
    "_ = vendor_df_val.pop('ORIGIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "latter-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UID Column\n",
    "uuids = intact_df.pop('UUID').values\n",
    "uuids_val = intact_df_val.pop('UUID').values\n",
    "\n",
    "_ = vendor_df.pop('UUID')\n",
    "_ = vendor_df_val.pop('UUID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "permanent-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training \tIntact Data: (5822, 43) Vendor: (5822, 91)\n",
      "Validation: \tIntact Data: (4000, 43) Vendor: (4000, 91)\n"
     ]
    }
   ],
   "source": [
    "## Sanity Check\n",
    "print(f\"Training \\tIntact Data: {str(intact_df.shape)} Vendor: {str(vendor_df.shape)}\")\n",
    "print(f\"Validation: \\tIntact Data: {str(intact_df_val.shape)} Vendor: {str(vendor_df_val.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-costs",
   "metadata": {},
   "source": [
    "### Define Dataloader Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valuable-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePartitionDataLoader(DataLoader):\n",
    "    \"\"\"DataLoader for a single vertically-partitioned dataset\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.collate_fn = id_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "appreciated-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalDataLoader:\n",
    "    \"\"\"Dataloader which batches data from a complete\n",
    "    set of vertically-partitioned datasets\n",
    "    i.e. the images dataset AND the labels dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data1, data2, *args, **kwargs):\n",
    "\n",
    "        self.dataloader1 = SinglePartitionDataLoader(\n",
    "            data1, *args, **kwargs\n",
    "        )\n",
    "        self.dataloader2 = SinglePartitionDataLoader(\n",
    "            data2, *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Zip Dataloaders \n",
    "        \"\"\"\n",
    "        return zip(self.dataloader1, self.dataloader2)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return length of dataset\n",
    "        \"\"\"\n",
    "        return (len(self.dataloader1) + len(self.dataloader2)) // 2\n",
    "\n",
    "    def drop_non_intersecting(self, intersection):\n",
    "        \"\"\"Remove elements and ids in the datasets that are not in the intersection.\"\"\"\n",
    "        self.dataloader1.dataset.data = self.dataloader1.dataset.data[intersection]\n",
    "        self.dataloader1.dataset.ids = self.dataloader1.dataset.ids[intersection]\n",
    "\n",
    "        self.dataloader1.dataset.labels = self.dataloader1.dataset.labels[intersection]\n",
    "        self.dataloader2.dataset.ids = self.dataloader2.dataset.ids[intersection]\n",
    "\n",
    "    def sort_by_ids(self) -> None:\n",
    "        \"\"\"\n",
    "        Sort each dataset by ids\n",
    "        \"\"\"\n",
    "        self.dataloader1.dataset.sort_by_ids()\n",
    "        self.dataloader2.dataset.sort_by_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-rachel",
   "metadata": {},
   "source": [
    "## Initialize Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "drawn-economics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (5822,) (5822, 42) (5822,)\n",
      "validation (4000,) (4000, 42) (4000,)\n"
     ]
    }
   ],
   "source": [
    "# Intact Dataset\n",
    "\n",
    "TARGET_COLUMN = \"CARAVAN\"\n",
    "\n",
    "#Training\n",
    "intact_labels = np.array(intact_df.pop(TARGET_COLUMN))\n",
    "intact_data = np.array(intact_df)\n",
    "print(\"train\", uuids.shape, intact_data.shape, intact_labels.shape)\n",
    "intact_dim = intact_data.shape[1]\n",
    "intact_dataset = VerticalDataset(ids=uuids, data=intact_data, labels=intact_labels)\n",
    "\n",
    "#Validation\n",
    "intact_labels_val = np.array(intact_df_val.pop(TARGET_COLUMN))\n",
    "intact_data_val = np.array(intact_df_val)\n",
    "print(\"validation\", uuids_val.shape, intact_data_val.shape, intact_labels_val.shape)\n",
    "intact_dataset_val = VerticalDataset(ids=uuids_val, data=intact_data_val, labels=intact_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exciting-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5822, 91)\n"
     ]
    }
   ],
   "source": [
    "# Vendor Dataset\n",
    "\n",
    "#Training\n",
    "vendor_data = np.array(vendor_df)\n",
    "print(vendor_data.shape)\n",
    "vendor_dim = vendor_data.shape[1]\n",
    "vendor_feat_dim = 4\n",
    "vendor_dataset = VerticalDataset(ids=uuids, data=vendor_data, labels=None)\n",
    "\n",
    "#Validation\n",
    "vendor_dataset_val = np.array(vendor_df_val)\n",
    "vendor_dataset_val = VerticalDataset(ids=uuids_val, data=vendor_dataset_val, labels=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-piece",
   "metadata": {},
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cheap-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Train Dataloader \n",
    "dataloader = VerticalDataLoader(intact_dataset, vendor_dataset, batch_size=512)\n",
    "\n",
    "# Compute private set intersection\n",
    "client_items = dataloader.dataloader1.dataset.get_ids()\n",
    "server_items = dataloader.dataloader2.dataset.get_ids()\n",
    " \n",
    "client = Client(client_items)\n",
    "server = Server(server_items)\n",
    "\n",
    "setup, response = server.process_request(client.request, len(client_items))\n",
    "intersection = client.compute_intersection(setup, response)\n",
    "\n",
    "# Order data\n",
    "dataloader.drop_non_intersecting(intersection)\n",
    "dataloader.sort_by_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "satisfied-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Validation Dataloader \n",
    "val_dataloader = VerticalDataLoader(intact_dataset_val, vendor_dataset_val, batch_size=512)\n",
    "\n",
    "# Compute private set intersection\n",
    "val_client_items = val_dataloader.dataloader1.dataset.get_ids()\n",
    "val_server_items = val_dataloader.dataloader2.dataset.get_ids()\n",
    "\n",
    "val_client = Client(val_client_items)\n",
    "val_server = Server(val_server_items)\n",
    "\n",
    "val_setup, val_response = val_server.process_request(val_client.request, len(val_client_items))\n",
    "val_intersection = val_client.compute_intersection(val_setup, val_response)\n",
    "\n",
    "# Order data\n",
    "val_dataloader.drop_non_intersecting(val_intersection)\n",
    "val_dataloader.sort_by_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-ceiling",
   "metadata": {},
   "source": [
    "## **Model Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntactModel(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    Model for the Intact dataset\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dim: \n",
    "        Dimensionality of Intact Data\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Intact Model\n",
    "    \"\"\"\n",
    "    def __init__(self, intact_dim, vendor_dim): \n",
    "        super(IntactModel, self).__init__()\n",
    "        self.fused_input_dim = intact_dim + vendor_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.fused_input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, intact_feat, vendor_feat):\n",
    "        feat = torch.cat([intact_feat, vendor_feat], dim=1)\n",
    "        pred = self.layers(feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VendorModel(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    Model for Vendor variables\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dim: \n",
    "        Dimensionality of the vendor data\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Credit Bureau Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vendor_dim): \n",
    "        super(VendorModel, self).__init__()\n",
    "        self.vendor_dim = vendor_dim\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Linear(self.vendor_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, vendor_feat):\n",
    "        pred = self.layers(vendor_feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    \"\"\"\n",
    "    A class representing SplitNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    intact_model:  \n",
    "        Home Credit Neural Network Module\n",
    "\n",
    "    vendor_model:   \n",
    "        Credit Bureau Neural Network Module\n",
    "\n",
    "    intact_opt:  \n",
    "        Optimizer for the Home Credit Neural Network Module\n",
    "\n",
    "    vendor_model:   \n",
    "        Optimizer for the Credit Bureau Neural Network Module\n",
    "\n",
    "    data: \n",
    "        A list storing intermediate computations at each index\n",
    "\n",
    "    remote_tensors: \n",
    "        A list storing intermediate computations at each index (Computation from each model detached from global computation graph)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the SplitNN\n",
    "\n",
    "    backward(): \n",
    "        Performs a backward pass through the SplitNN\n",
    "\n",
    "    zero_grads():\n",
    "        Zeros the gradients of all networks in SplitNN\n",
    "\n",
    "    step():\n",
    "        Updates the parameters of all networks in SplitNN\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, intact_model, vendor_model, intact_opt, vendor_opt):\n",
    "        self.intact_model = intact_model\n",
    "        self.vendor_model = vendor_model\n",
    "        self.intact_opt = intact_opt\n",
    "        self.vendor_opt = vendor_opt\n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "\n",
    "    def forward(self, intact_x, vendor_x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:  \n",
    "            Input Sample \n",
    "        \"\"\"\n",
    "\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "        # Forward pass through first model\n",
    "        data.append(self.vendor_model(vendor_x))\n",
    "\n",
    "        # if location of data is the same as location of the subsequent model\n",
    "        if data[-1].location == self.intact_model.location:\n",
    "            # store computation in remote tensor array \n",
    "            # Gradients will be only computed backward upto the point of detachment\n",
    "            remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "        else:\n",
    "            # else move data to location of subsequent model and store computation in remote tensor array \n",
    "            # Gradients will be only computed backward upto the point of detachment\n",
    "            remote_tensors.append(\n",
    "                data[-1].detach().move(self.intact_model.location).requires_grad_()\n",
    "            )\n",
    "\n",
    "        # Get and return final output of model\n",
    "        data.append(self.intact_model(intact_x, remote_tensors[-1]))\n",
    "\n",
    "        self.data = data \n",
    "        self.remote_tensors = remote_tensors\n",
    "        return data[-1]\n",
    "\n",
    "    def backward(self):\n",
    "        # if location of data is the same as detatched data \n",
    "        if self.remote_tensors[0].location == self.data[0].location:\n",
    "            # Store gradients from remote_tensor \n",
    "            grads = self.remote_tensors[0].grad.copy()\n",
    "        else:\n",
    "            # Move gradients to lovation of Store grad\n",
    "            grads = self.remote_tensors[0].grad.copy().move(self.data[0].location)\n",
    "\n",
    "        self.data[0].backward(grads)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.vendor_opt.zero_grad()\n",
    "        self.intact_opt.zero_grad()\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.vendor_opt.step()\n",
    "        self.intact_opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-assistant",
   "metadata": {},
   "source": [
    "### Initialize and Configure Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training globals \n",
    "epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Iniatialize Intact Model and Optimizer\n",
    "intact_model = IntactModel(intact_dim, vendor_feat_dim)\n",
    "intact_opt = torch.optim.Adam(intact_model.parameters(), lr=.001,  betas=(0.9, 0.999))\n",
    "\n",
    "# Iniatialize Credit Bureau Model and Optmizer\n",
    "vendor_model = VendorModel(vendor_dim)\n",
    "vendor_opt = torch.optim.Adam(vendor_model.parameters(), lr=.001,  betas=(0.9, 0.999))\n",
    "\n",
    "# Define Split Neural Network\n",
    "splitNN = SplitNN(intact_model, vendor_model, intact_opt, vendor_opt)\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-czech",
   "metadata": {},
   "source": [
    "### Configure (Virtual) Remote Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some workers\n",
    "intact_worker = sy.VirtualWorker(hook, id=\"intact\")\n",
    "vendor_worker = sy.VirtualWorker(hook, id=\"vendor\")\n",
    "\n",
    "# Send Model Segments to model locations\n",
    "model_locations = [intact_worker, vendor_worker]\n",
    "models = [intact_model, vendor_model]\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-frontier",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataloader, splitNN):\n",
    "    running_loss = 0\n",
    "    for (intact_data, labels, id1), (vendor_data, id2) in dataloader:\n",
    "        # Send data and labels to machine model is on\n",
    "        labels = labels.float()\n",
    "        intact_data = intact_data.send(intact_model.location)\n",
    "        labels = labels.send(intact_model.location)\n",
    "        vendor_data = vendor_data.send(vendor_model.location)\n",
    "\n",
    "        # Zero our grads\n",
    "        splitNN.zero_grads()\n",
    "    \n",
    "        # Make a prediction\n",
    "        pred = splitNN.forward(intact_data, vendor_data).squeeze()\n",
    "\n",
    "        # Figure out how much we missed by\n",
    "        loss = criterion(pred, labels)\n",
    "    \n",
    "        # Backprop the loss on the end layer\n",
    "        loss.backward()\n",
    "        splitNN.backward()\n",
    "    \n",
    "        # Change the weights\n",
    "        splitNN.step()\n",
    "        \n",
    "        # Accumulate Loss\n",
    "        running_loss += loss.get()\n",
    "\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(val_dataloader, splitNN):\n",
    "    running_loss = 0\n",
    "    exs = 0 \n",
    "    correct = 0\n",
    "    aucs = []\n",
    "    f1s = []\n",
    "    for (intact_data_val, labels_val, id1), (vendor_data_val, id2) in val_dataloader:\n",
    "        # Send data and labels to machine model is on\n",
    "        labels_val  = labels_val.float()\n",
    "        intact_data_val = intact_data_val.send(intact_model.location)\n",
    "        labels_val  = labels_val.send(intact_model.location)\n",
    "        vendor_data_val = vendor_data_val.send(vendor_model.location)\n",
    "    \n",
    "        # Make a prediction\n",
    "        with torch.no_grad():\n",
    "            pred = splitNN.forward(intact_data_val, vendor_data_val).squeeze()\n",
    "        \n",
    "        #Calcualte Loss\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        loss = criterion(pred, labels_val)\n",
    "\n",
    "        #Calculate AUC\n",
    "        thresh_pred = (pred > .5).float()\n",
    "        thresh_pred = thresh_pred.get().int()\n",
    "        labels_val = labels_val.get().int()\n",
    "\n",
    "        # Fix Me: Undefined for batches with all-same labels...\n",
    "        auc = roc_auc_score(labels_val, pred.get().numpy())\n",
    "        f1 = f1_score(labels_val, thresh_pred)\n",
    "\n",
    "        #Calculate Accuracy Components\n",
    "        num_exs = intact_data_val.shape[0]\n",
    "        num_correct = torch.sum(thresh_pred == labels_val).item()\n",
    "\n",
    "        # Accumulate loss, accuracy and auc\n",
    "        exs += num_exs\n",
    "        correct += num_correct\n",
    "        running_loss += loss.get()\n",
    "        aucs.append(auc)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    auc = np.mean(np.array(aucs))\n",
    "    f1 = np.mean(np.array(f1s))\n",
    "    accuracy = correct / exs\n",
    "\n",
    "    return f1, accuracy, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"Train Loss\", \"Validation Loss\", \"Accuracy\", \"F1\"]\n",
    "metrics = {metric:[] for metric in metric_names}\n",
    "\n",
    "# Train Loop\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Train Step \n",
    "    train_loss = train_step(dataloader, splitNN)\n",
    "\n",
    "    # Train Step\n",
    "    f1, accuracy, val_loss = val_step(val_dataloader, splitNN)\n",
    "    \n",
    "    # Log metrics\n",
    "    print(f\"Epoch: {i} \\t F1: {f1}\")\n",
    "    metrics[\"Train Loss\"].append(train_loss.item())\n",
    "    metrics[\"Validation Loss\"].append(val_loss.item())\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"F1\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pets_ALL",
   "language": "python",
   "name": "pets_all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
