{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gDUrA30qZ_Y"
   },
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsFwOPMikD0z"
   },
   "source": [
    "The following notebook provides a comprehensive overview and implementation of the [Private Aggregation of Teacher Ensembles (PATE)](https://arxiv.org/pdf/1610.05755.pdf) using [PyTorch](https://pytorch.org/) and [PySyft](https://github.com/OpenMined/PySyft). The dataset that is used is [Home Credit Default Dataset](https://www.kaggle.com/c/home-credit-default-risk/overview) which contains features about credit applicants drawn from the internal operations of a financial insititution as well as corresponding Credit Bureau data. The target is whether or not the appliant will default on their debt. The following sections contain a detailed a theoretical overview and subsequent implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UJ17d9wi_Wj"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSw3SY9KH7ER"
   },
   "source": [
    "## Private AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGDRyW8wktJ3"
   },
   "source": [
    "### Introduction to Private AI\n",
    "A major concern of production machine learning systems is maintaining the privacy of the data that was used to train it. This is especially relevant in use cases with private and sensitive information. In the Private AI literature, a party looking to reveal information about the data that a model is trained on is called an adversary. Even if the adversary does not have access to the data explicitly, access to the model and/or its outputs can reveal information about the data that the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6CDtTTYkxLk"
   },
   "source": [
    "### Privacy Attacks\n",
    "An adversary employs a privacy attack in order to reveal information about the data that the model was trained on. Two attacks that are well researched in the Private AI Literature: \n",
    "- **Model Inversion Attack:** Try to obtain information about typical samples in the training dataset. This involves attacks that euther reconst a specific sample or representative samples in the training dataset. \n",
    "\n",
    "- **Membership Inference Attack:** Try to ascertain whether or not a sample was used to train a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIykJlBGk3ye"
   },
   "source": [
    "### Threat Models\n",
    "Each adversary has a threat model that informs the manner in which they go about a privacy attack. The threat model describes the level of access that an adverary has to a model. At a high level, there are two threat models: \n",
    "- **Black Box Adversary:** Solely able to query the model. Thus, the only information made available to the adversary is the inputs and outputs of the model. \n",
    "- **White Box Adversary:** The adversary is able to query the model as well as have access to its internal parameters. This implies that the adversary has access to the input, output and the intermediate computations made by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al6B9oHXC4sq"
   },
   "source": [
    "## Differential Privacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp4s6GL0_gJt"
   },
   "source": [
    "### Introduction to Differential Privacy\n",
    "In order to evaluate the robustness of a model to privacy attacks, we have to define a framework through which we can obtain a quantitative measure that describes its performance in terms of privacy. To this end, Differential Privacy has been proposed and offers a powerful mechanism to assess and rank the privacy of models. It does so based on the sensitivity of a model to the inclusion of a specific sample.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"433\" alt=\"Screen Shot 2021-09-28 at 5 41 20 PM\" src=\"https://user-images.githubusercontent.com/34798787/137939981-a9968386-4a28-4447-ad9e-5babab497eeb.png\">  \n",
    "</p>\n",
    "<center>\n",
    "<a href=https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our>Source</a>  \n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "Specified formally, the definition of differential privacy is given by the below inequality: \n",
    "<p align=\"center\">\n",
    "<img width=\"185\" alt=\"Screen Shot 2021-10-19 at 11 40 16 AM\" src=\"https://user-images.githubusercontent.com/34798787/137944948-cffd9db8-5a29-4f98-bf5a-b5b7d4bbe25b.png\">\n",
    "</p>\n",
    "\n",
    "where M is the model, x is orginal the dataset, x' is the original dataset augmented to include or exclude a single sample. $\\epsilon$ acts as a metric of privacy loss based on a differential change in data. The smaller the value is, the better privacy protection. Rearranging the inequality yields:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"345\" alt=\"Screen Shot 2021-10-19 at 3 37 50 PM\" src=\"https://user-images.githubusercontent.com/34798787/137978800-d3c56d46-5b7c-4b37-a61e-6f7b8daa61d2.png\">\n",
    "</p>\n",
    "This is a strong to guarentee to achieve in practice so a failure probability in order to relax this constraint we add a failure probability, $\\delta$, to the RHS of the inequality: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"400\" alt=\"Screen Shot 2021-10-19 at 3 35 50 PM\" src=\"https://user-images.githubusercontent.com/34798787/137978567-7a0313bc-d9cd-4a0d-9f9b-7df3711f2c3d.png\">\n",
    "</p>\n",
    "\n",
    "As long as delta is smaller than the probability that a sample occurs in the dataset, we will still obtain a high degree of privacy. This allows us to relax the guarentee we need to provide while maintaining an acceptable level of privacy. As parameters that define the interval of differential privacy, a model with $\\epsilon$ and $\\delta$ is ($\\epsilon$, $\\delta$)-differentially private. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGFoQdWKnhO4"
   },
   "source": [
    "### Composition in Differential Privacy\n",
    "An important consideration in practice is determining the differential privacy of a composition of a number of models. Drawing from [The Algorithmic Foundations\n",
    "of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf), the composition of k differentially private mechanisms, where the $i$th mechanism is ($\\epsilon_{i}$, $\\delta_{i}$)-differentially private, for 1 $\\leq$ i $\\leq$ k, is ($\\sum \\epsilon_{i}$, $\\sum \\delta_{i}$)-differentially private.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AUvaoZwo5wA"
   },
   "source": [
    "### Privacy Amplication Theorem\n",
    "If we select a subset of samples from the dataset, it is intuitive that we would incur a privacy loss that is lower than that of using the entire dataset. This is the essence of the Privacy Amplication Theorem which states that if we randomly sample a q of the data, rather than the entrie dataset, then an ($\\epsilon$, $\\delta$) private mechansm becomes ($q\\epsilon$, $q\\delta$)-differentially private.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSBhhCsmo-PK"
   },
   "source": [
    "### Fundamental Law of Information Recovery\n",
    "The Fundamental Law of Information Recovery states that overly accurate estimates of too many statistics erodes the privacy of data. This implies that continuously querying a private mechanism will increase the privacy loss incurred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2H6fxx_pCBy"
   },
   "source": [
    "### Implications for Training and Testing Machine Learning Models\n",
    "Within the context of training and testing a machine learning model, we can use Composition in Differentially Privacy and Privacy Amplication Theorem to derive the privacy loss in situations where we are iteritvely querying a model with a random subset of the data, such as in the case of training and testing Neural Networks. The summation of privacy losses over queries follows the intuition from the Fundamental Law of Information Recovery that continuously querying a private mechanism will increase the privacy loss incurred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbmYkT6eDVbb"
   },
   "source": [
    "## Differentially Private Stochastic Gradient Descent (DP-SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdshKgqDDVX6"
   },
   "source": [
    "### Introduction to DP-SGD\n",
    "The seminal paper that applies Differential Privacy to Deep Learning is [Deep Learing with Differential Privacy](https://arxiv.org/abs/1607.00133). In this paper, a differentially-private variant of stochastic gradient descent is proposed (DP-SGD). \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"dpsgd\" src=\"https://user-images.githubusercontent.com/34798787/138136971-c51261d3-73de-4afb-97b9-71e149ddfacd.png\">\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<a href=https://secml.github.io/class4>Source</a>  \n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLuoqxJupmti"
   },
   "source": [
    "### Estimating Differential Privacy for a Single Gradient Update\n",
    "As in vanilla SGD, we start with calculating the gradient of a batch of data. In order to limit the amount information we learn from the batch, we clip the gradient at C which is a hyperpareter for the algorithim. We than add noise $\\sigma^2$ proportional C and than update the parameters of the model. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sT7fO8Bpt3-"
   },
   "source": [
    "### Estimating Differential Privacy for accross Gradient Updates\n",
    "In order to get the total privacy loss, we must aggregate the privacy loss for each gradient update. Based on Composition in Differential Privacy and The Privacy Amplicaiton Theorem, we can define a Naive estimate of the upper bound of the privacy loss. Given an ($\\epsilon$, $\\delta$)-differentially private algorithim, with a batch size proportional to $q$ run for T iterations has a differential privacy of ($Tq\\epsilon$, $Tq\\delta$). \n",
    "\n",
    "It turns out that a more complex analysis can yield lower bounds for the privacy loss. Thus, finding the lowest bound possible allows us to more accurately determine the privacy loss and to avoid overstimating it. For example, the Strong Compositon Theorem can be used to prove an even lower bound on the privacy loss. In the DP-SGD paper, the authors propose the Moments Accountant which provided the lowest bound \n",
    "- **Naive Analysis:** ($Tq\\epsilon$, $Tq\\delta$)-differentially private\n",
    "- **Strong Composition Theorem:** ($O(q\\epsilon\\sqrt{Tlog(1/\\delta)})$, $Tq\\delta$)-differentially private\n",
    "- **Moments Accountant:** ($O(q\\epsilon\\sqrt{T})$, $\\delta$)-differentially private.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx263fuIpysx"
   },
   "source": [
    "### Moments Accountant\n",
    "The fundamental insight of the Moments Accountant technique is that the privacy loss is a random variable. Thus, if we look at the distribution of the privacy loss, we can see that $\\epsilon$ defines the privacy budget for the loss and $\\delta$ provides an upper bound on the tail of the distribution. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"Screen Shot 2021-10-20 at 1 50 02 PM\" src=\"https://user-images.githubusercontent.com/34798787/138145128-0d67885f-a5c4-448f-9fcd-78fa0452a3ce.png\">\n",
    "</p>\n",
    "<center>\n",
    "<a href=https://www.youtube.com/watch?v=jm1Sfdno_5A>Source</a>  \n",
    "</center>\n",
    "\n",
    "By treating the privacy loss as a random variable, we can leverage probability theory to derive a lower bound on the privacy loss. Specifically, the Moments Accountant Technique uses the moments of the distribution (ie mean, variance, skewness and kurtosis) in order to do so. Incorporating the higher order information of the distibution made available through the moments  allows us to derive a lower bound on the privacy loss: $O(q\\epsilon\\sqrt{T})$, $\\delta$). For a deeper look into the Moments Accountant technique, refer to [Deep Learing with Differential Privacy](https://arxiv.org/abs/1607.00133).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6HDEC24D2mA"
   },
   "source": [
    "## Private Aggregation of Teacher Ensembles (PATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CXDPUynGRC1"
   },
   "source": [
    "### Introduction to PATE\n",
    "In [Scalable Private Learning with PATE](https://arxiv.org/abs/1802.08908), authors set out solve the problem of preserving the privacy of training data when training classfiers. They began with defining certain criteria for the solution:\n",
    "- Differential privacy protection guarentees\n",
    "- Intuitive privacy protection guarentees \n",
    "- **Independent of learning algorithim**\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"dpsgd\" src=\"https://user-images.githubusercontent.com/34798787/139095511-ff27899c-80f0-45bf-a4d0-d18820c20e34.png\">\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<a href=https://www.youtube.com/watch?v=cjo_u_yT2wQ&t=1s>Source</a>  \n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WmZJ5keqNei"
   },
   "source": [
    "### Ensembling\n",
    "\n",
    "In differential privacy, we seek to learn general trends from the data. That is, the outcome of the the prediction of a sample, should be the same whether or not we choose to include an sample in the dataset for each sample in the dataset. One natural way to achieve this to use an ensemble of models trained on random subsets of the dataset. In this way, the prediction for a specific sample is less likely to depend on a single example. Furthermore, if there is a strong consensus among the ensemble, it is likely the prediction stems from a general trend rather than a specific sample. In practice, this allows us to define a lower, data dependent bound on the privacy loss. \n",
    "\n",
    "Although ensembling enhances differentially privacy, it is still possible that the inclusion/exclusion of samples in the various teachers will change the outcome of the predictions. This is especially the case when there is not a strong consensus among the ensemble; the vote of single teacher can sway the prediction of the output. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aibw7NVzqQ5P"
   },
   "source": [
    "### Noisy Aggregation\n",
    "\n",
    "In order to address the aforementioned shortcoming, we can aggregate the votes of the teachers in a noisy way. This is realized by adding noise to the final prediction.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"dpsgd\" src=\"https://user-images.githubusercontent.com/34798787/139089966-fa783578-cc52-4a58-bcb4-9a06432868ac.png\">\n",
    "</p>\n",
    "<center>\n",
    "<a href=https://www.youtube.com/watch?v=cjo_u_yT2wQ&t=1s>Source</a>  \n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sITlMBDZqTnt"
   },
   "source": [
    "### Student Training\n",
    "Although the aggregated teacher is a good step towards differential privacy, it does have some shortcomings: \n",
    "- Each prediction increases total privacy loss. Thus, the privacy budget creates a tradeoff between the accuracy and number of predictions.\n",
    "- Inspection of internal may reveal private data. However, we want privacy guarentees that hold for white box adversaries. \n",
    "\n",
    "As a result, the PATE framework introduces a student network that is trained on publicly available data using labels from Teacher Model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"dpsgd\" src=\"https://user-images.githubusercontent.com/34798787/138165916-a61f044d-4d45-4f5f-8e62-3fca94b65b84.png\">\n",
    "</p>\n",
    "<center>\n",
    "<a href=https://www.youtube.com/watch?v=cjo_u_yT2wQ&t=1s>Source</a>  \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYzZpovT72cA"
   },
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmQzmmT33iZT"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The following demo consists of three sections:\n",
    "- **Data Preparation** \n",
    "    - Define Dataset\n",
    "    - Define Dataloader\n",
    "- **Model Preparation** \n",
    "    - Model Defintion\n",
    "- **Training and Validation**\n",
    "    - Training Teacher Ensemble\n",
    "    - Generating Labelled Public Dataset\n",
    "    - Train and Evaluate Student Model\n",
    "- **Interpret Results** \n",
    "    - Accuarcy \n",
    "    - Privacy Loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nWcWDv27-7Z"
   },
   "source": [
    "## Environment Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "Hh3hR-ZM8RZb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from syft.frameworks.torch.dp import pate\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "1H8W-lQS-Fq5"
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = \"train.csv\"\n",
    "DATA_PATH = \"/ssd003/projects/pets/datasets/home_credit/train.csv\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_PERC = .75\n",
    "BATCH_SIZE = 1024\n",
    "NUM_TEACHERS = 7\n",
    "TEACHER_EPOCHS = 40\n",
    "STUDENT_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDPVc5OF8Diw"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhcsiNmAs9jN"
   },
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "bcmCSxdo-ktB"
   },
   "outputs": [],
   "source": [
    "class HomeCredit(Dataset): \n",
    "    \"\"\"Dataset for Vertical Federated Learning\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Numpy Array) : Numpy Array with Features\n",
    "            labels (Numpy Array) : Numpy Array with Labels. None if not available. \n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return record single record\"\"\"\n",
    "        features = self.data[idx].astype(np.float32)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return Length\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmIPkX2cEh_Y"
   },
   "source": [
    "### Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "3oA2TNmJItaX"
   },
   "outputs": [],
   "source": [
    "def get_loaders(data, num_teachers, batch_size):\n",
    "    \"\"\" \n",
    "    Function to create data loaders for the Teacher Class.\n",
    "    \n",
    "    :param data: Numpy Array of the data \n",
    "    :param num_teacher: Number of teacher models \n",
    "    :param batch_size: Batch size for the dataloaders\n",
    "\n",
    "    :return: Return teacher loaders and student loader (with actual labels)\n",
    "    \"\"\" \n",
    "    loaders = []\n",
    "    sample_size = len(data) // (num_teachers + 1)\n",
    "\n",
    "    for i in range(num_teachers):\n",
    "        indices = list(range(i*sample_size, (i+1)*sample_size))\n",
    "        subset_data = Subset(data, indices)\n",
    "        loader = DataLoader(subset_data, batch_size=batch_size)\n",
    "        loaders.append(loader)\n",
    "    \n",
    "    return loaders[1:], loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "j0iCzQxjhie0"
   },
   "outputs": [],
   "source": [
    "def student_loader(student_train_loader, labels):\n",
    "    \"\"\" \n",
    "    Function to modify the student loader to include labels from teacher\n",
    "    \n",
    "    :param student_train_loader: The student loader with actual labels \n",
    "    :param labels: Labels from the teacher model\n",
    "\n",
    "    :return: Return iterator  \n",
    "    \"\"\" \n",
    "    # Use teacher to label data (discard actual labels)\n",
    "    for i, (data, _) in enumerate(iter(student_train_loader)):\n",
    "        yield data, torch.from_numpy(labels[i*len(data): (i+1)*len(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "A7GlbTqt7-ay"
   },
   "outputs": [],
   "source": [
    "# Load Data and Labels\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "labels = data.pop(\"target\")\n",
    "data = data.to_numpy(dtype=np.float32)\n",
    "labels = labels.to_numpy(dtype=np.int)\n",
    "dataset = HomeCredit(data=data, labels=labels)\n",
    "\n",
    "# #OverSample\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# oversample = SMOTE()\n",
    "# X_train_oversampled, y_oversampled = oversample.fit_resample(dataset.data, dataset.labels)\n",
    "\n",
    "# # Get train and validation size\n",
    "# train_size = int(len(X_train_oversampled) * TRAIN_PERC)\n",
    "# val_size = len(X_train_oversampled) - train_size\n",
    "# train_data, val_data = random_split(X_train_oversampled, [train_size, val_size])\n",
    "\n",
    "# Get train and validation size\n",
    "train_size = int(len(dataset) * TRAIN_PERC)\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define dataloaders\n",
    "t_loaders, s_loader = get_loaders(train_data, NUM_TEACHERS, BATCH_SIZE) # Teacher loaders, student loader\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, drop_last=True) # Loader to validate in Train Ensemble and Train Student Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9Is-2FbS0IM"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "tlM7hrPHFNuz"
   },
   "outputs": [],
   "source": [
    "class HCModel(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    Model for Credit Bureau\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    feat_dim: \n",
    "        Dimensionality of Data\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Credit Bureau Model\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim): \n",
    "        super(HCModel, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.feat_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        pred = self.layers(feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "ZszYeRq7OA_U"
   },
   "outputs": [],
   "source": [
    "# Initialize models and otptimizers for teacher ensembles\n",
    "models = [HCModel(feat_dim=data.shape[1]) for i in range(NUM_TEACHERS)]\n",
    "opts = [torch.optim.Adam(model.parameters(), lr=.001,  betas=(0.9, 0.999)) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lxO0IZ0ocAd"
   },
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjwQkmg_CxHw"
   },
   "source": [
    "### Train Teacher Ensemble\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"dpsgd\" src=\"https://user-images.githubusercontent.com/34798787/139118910-63e067a7-1b03-4501-8f78-749c2012f608.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "wS7AVPgiZso_"
   },
   "outputs": [],
   "source": [
    "def train_models(num_teachers, models, opts, train_loaders, val_loader):\n",
    "    \"\"\" \n",
    "    Train the teacher models on the the training data and assess on validation set\n",
    "    \n",
    "    :param num_teacher: Number of teacher models \n",
    "    :param models A list of teacher models \n",
    "    :param opts A list of optimizers\n",
    "    :param train_loaders A list of train data loaders \n",
    "    :param val_loader A validation loader\n",
    "\n",
    "    :return: Return A list of train and validation losses for each epoch\n",
    "    \"\"\" \n",
    "    train_losses = [[] for i in range(num_teachers)]\n",
    "    val_losses = [[] for i in range(num_teachers)]\n",
    "    for epoch in range(TEACHER_EPOCHS):\n",
    "        train_loss = train_step(models, opts, train_loaders)\n",
    "        val_loss = val_step(models, val_loader)\n",
    "        avg_train_loss = sum(train_loss) / len(train_loss)\n",
    "        avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "        print(f\"Epoch: {str(epoch)}\\t AVG Train Loss: {str(avg_train_loss)}\\t AVG Val Loss: {str(avg_val_loss)}\")\n",
    "\n",
    "\n",
    "        for i in range(num_teachers):\n",
    "            train_losses[i].append(train_loss[i])\n",
    "            val_losses[i].append(val_loss[i])\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "-2zVDJU-6fp8"
   },
   "outputs": [],
   "source": [
    "def train_step(models, opts, train_loaders):\n",
    "    \"\"\" \n",
    "    Train teacher ensembles for a single epoch\n",
    "    \n",
    "    :param models A list of teacher models \n",
    "    :param opts A list of optimizers\n",
    "    :param train_loaders A list of train data loaders \n",
    "\n",
    "    :return: Return A list of train and validation losses for each teacher for each epoch\n",
    "    \"\"\" \n",
    "    train_running_losses = [0 for i in range(len(models))]\n",
    "    for i, (model, opt, loader) in enumerate(zip(models, opts, train_loaders)):\n",
    "            model = model.to(DEVICE)\n",
    "            for feat, lbl in loader:\n",
    "                feat, lbl = feat.to(DEVICE), lbl.to(DEVICE)\n",
    "                model.zero_grad()\n",
    "                p = model(feat)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                loss = criterion(out, lbl)\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                train_running_losses[i] += loss.detach().cpu().item()\n",
    "    \n",
    "    return train_running_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "otY7UT5F62Gy"
   },
   "outputs": [],
   "source": [
    "def val_step(models, loader):\n",
    "    \"\"\" \n",
    "    Validation teacher ensembles for a single epoch\n",
    "    \n",
    "    :param models A list of teacher models \n",
    "    :param loader Validation dataloader\n",
    "\n",
    "    :return: A list of validation losses\n",
    "    \"\"\" \n",
    "    val_loss = []\n",
    "    for i, model in enumerate(models):\n",
    "        outputs, labels, _ = predict(model, loader)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss.append(loss.cpu().item())\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "RnZn_nQJnAso"
   },
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    \"\"\" \n",
    "    Get predictions of single model on loader\n",
    "    \n",
    "    :param model A teacher model \n",
    "    :param loader A dataloader\n",
    "\n",
    "    :return: output of the model, labels, index of predicted class\n",
    "    \"\"\" \n",
    "    preds = torch.zeros(0, dtype=torch.long).to(DEVICE)\n",
    "    labels = torch.zeros(0, dtype=torch.long).to(DEVICE)\n",
    "    outputs = []\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    for feat, lbl in loader:\n",
    "        feat, lbl = feat.to(DEVICE), lbl.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(feat)\n",
    "        outputs.append(output)\n",
    "        ps = torch.argmax(torch.exp(output), dim=1)\n",
    "        preds = torch.cat((preds, ps))\n",
    "        labels = torch.cat((labels, lbl))\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "    return outputs, labels, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VN4Rwq7fDjs9"
   },
   "source": [
    "### Generating Labelled Public Dataset\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"dpsgd\" src=\"https://user-images.githubusercontent.com/34798787/139119093-427aee59-8883-478d-85a6-7bd13831068d.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "61VQB0RPpzuC"
   },
   "outputs": [],
   "source": [
    "def agg_teacher(models, loader, epsilon):\n",
    "    \"\"\" \n",
    "    Get noisily aggregated prediction of teacher ensemble \n",
    "    \n",
    "    :param models A list of teacher models \n",
    "    :param loader A dataloader\n",
    "    :param epsilon A noise parameter for the laplace \n",
    "\n",
    "    :return: index of predictions, actual labels\n",
    "    \"\"\" \n",
    "    preds = []\n",
    "    for i, model in enumerate(models):\n",
    "        _, _, pred = predict(model, loader)\n",
    "        preds.append(pred.cpu().numpy())\n",
    "    preds = np.stack(preds)\n",
    "\n",
    "    labels = np.array([]).astype(int)\n",
    "    for pred in np.transpose(preds):\n",
    "        label_counts = np.bincount(pred, minlength=2)\n",
    "        beta = 1 / epsilon\n",
    "\n",
    "        for i in range(len(label_counts)):\n",
    "            noise = np.random.laplace(0, beta, 1)\n",
    "            label_counts[i] += noise\n",
    "\n",
    "        new_label = np.argmax(label_counts)\n",
    "        labels = np.append(labels, new_label)\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tI8PMOaCD1hf"
   },
   "source": [
    "### Train and Validate Student\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"dpsgd\" src=\"https://user-images.githubusercontent.com/34798787/139119816-3f0d2c3a-dae8-49a3-bbf8-0e4ea614b284.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "r0zs6v0fI_D5"
   },
   "outputs": [],
   "source": [
    "def train_student(model, opt, train_loader, val_loader, epochs):\n",
    "    \"\"\" \n",
    "    Train student on public dataset labelled with teach ensemble\n",
    "    \n",
    "    :param model Student model\n",
    "    :param opt Optimizer for the student model \n",
    "    :param train_loader Train dataloader\n",
    "    :param val_loader Validation dataloader\n",
    "    :epochs The number of epochs\n",
    "\n",
    "    :return: list of train losses, list of validation losses, list of validation aucs\n",
    "    \"\"\" \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_student_step(model, opt, train_loader)\n",
    "        val_loss, val_auc = val_student_step(model, val_loader)\n",
    "        print(f\"{str(epoch)}\\t AVG Train Loss: {str(train_loss)}\\t AVG Val Loss: {str(val_loss)} \\t AVG AUC: {val_auc}\")\n",
    " \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "    return train_losses, val_losses, val_aucs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "V4qFDbOBJcm3"
   },
   "outputs": [],
   "source": [
    "def train_student_step(model, opt, train_loader):\n",
    "    \"\"\" \n",
    "    Train step on student model\n",
    "    \n",
    "    :param model Student model\n",
    "    :param opt Optimizer for hte student model \n",
    "    :param train_loader Train dataloader\n",
    "\n",
    "\n",
    "    :return: running loss from the step \n",
    "    \"\"\"\n",
    "    running_loss = 0\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    for i, (feat, lbl) in enumerate(train_loader):\n",
    "        feat, lbl = feat.to(DEVICE), lbl.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        out = model(feat)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(out, lbl)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "osDFv-eMLEgj"
   },
   "outputs": [],
   "source": [
    "def val_student_step(model, val_loader):\n",
    "    \"\"\" \n",
    "    Validation step on student model\n",
    "    \n",
    "    :param model Student model\n",
    "    :param val_loader A validation load\n",
    "\n",
    "\n",
    "    :return: Average loss , Average auc\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    aucs = []\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    for i, (feat, lbl) in enumerate(val_loader):\n",
    "        feat, lbl = feat.to(DEVICE), lbl.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(feat)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(out, lbl)\n",
    "        auc = roc_auc_score(lbl.cpu().numpy(), out.cpu().numpy()[:, 1])\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        aucs.append(auc)\n",
    "\n",
    "    avg_auc = sum(aucs) / len(aucs)\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss, avg_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upTQ6_vwkHEi",
    "outputId": "b09d1db9-391c-434e-c6a5-0ad1f1f5e954"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1667/955430819.py\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(num_teachers, models, opts, train_loaders, val_loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_teachers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEACHER_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1667/4196186056.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(models, opts, train_loaders)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_losses, val_losses = train_models(NUM_TEACHERS, models, opts, t_loaders, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpWaPKnqveEs",
    "outputId": "1dd13389-1ff0-4683-8900-baf39997b1a7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 28829) (28829,)\n"
     ]
    }
   ],
   "source": [
    "outputs, s_labels = agg_teacher(models, s_loader, 0.001) \n",
    "print(outputs.shape, s_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.1\n",
      "0.15000000000000002\n",
      "0.2\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "epsilons = np.arange(0.05,0.3,0.05)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_auc = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    outputs, s_labels = agg_teacher(models, s_loader, epsilon) \n",
    "    print(outputs.shape, s_labels.shape)\n",
    "\n",
    "    student_model = HCModel(feat_dim=104)\n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=0.003)\n",
    "\n",
    "    train_loss, val_loss, val_auc  = train_student(student_model, optimizer, s_loader, val_loader, STUDENT_EPOCHS)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_auc.append(val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbdl5eqYvj08",
    "outputId": "a9a0eaca-9244-49cf-ad92-15f0559c52ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t AVG Train Loss: 16.506971210241318\t AVG Val Loss: 0.4252050896485647 \t AVG AUC: 0.5099518350444285\n",
      "1\t AVG Train Loss: 11.534509301185608\t AVG Val Loss: 0.3957475010553996 \t AVG AUC: 0.5686575849463306\n",
      "2\t AVG Train Loss: 11.355411797761917\t AVG Val Loss: 0.3953911244869232 \t AVG AUC: 0.571486560603037\n",
      "3\t AVG Train Loss: 11.350496381521225\t AVG Val Loss: 0.39530982573827106 \t AVG AUC: 0.5708166202844558\n",
      "4\t AVG Train Loss: 11.348618686199188\t AVG Val Loss: 0.39526206652323403 \t AVG AUC: 0.5702661590463183\n",
      "5\t AVG Train Loss: 11.347353219985962\t AVG Val Loss: 0.3952302304903666 \t AVG AUC: 0.5699358480355371\n",
      "6\t AVG Train Loss: 11.346402287483215\t AVG Val Loss: 0.3952086834112803 \t AVG AUC: 0.5699224223516385\n",
      "7\t AVG Train Loss: 11.345820754766464\t AVG Val Loss: 0.3951903720696767 \t AVG AUC: 0.5699500905631122\n",
      "8\t AVG Train Loss: 11.345166951417923\t AVG Val Loss: 0.39517708897590637 \t AVG AUC: 0.5700647720351318\n",
      "9\t AVG Train Loss: 11.344734340906143\t AVG Val Loss: 0.3951695704460144 \t AVG AUC: 0.5705075814507912\n",
      "10\t AVG Train Loss: 11.344323992729187\t AVG Val Loss: 0.3951650357246399 \t AVG AUC: 0.5714274059054911\n",
      "11\t AVG Train Loss: 11.34384498000145\t AVG Val Loss: 0.39515742103258766 \t AVG AUC: 0.5725721142609131\n",
      "12\t AVG Train Loss: 11.343482285737991\t AVG Val Loss: 0.39515675107638043 \t AVG AUC: 0.5736928286869714\n",
      "13\t AVG Train Loss: 11.343225061893463\t AVG Val Loss: 0.3951520526409149 \t AVG AUC: 0.5744887473332647\n",
      "14\t AVG Train Loss: 11.34274274110794\t AVG Val Loss: 0.3951487119992574 \t AVG AUC: 0.5751149324207466\n",
      "15\t AVG Train Loss: 11.342446863651276\t AVG Val Loss: 0.3951503658294678 \t AVG AUC: 0.5757008095440548\n",
      "16\t AVG Train Loss: 11.34235605597496\t AVG Val Loss: 0.3951485466957092 \t AVG AUC: 0.5766274364438458\n",
      "17\t AVG Train Loss: 11.34186664223671\t AVG Val Loss: 0.395153333346049 \t AVG AUC: 0.5775212983489868\n",
      "18\t AVG Train Loss: 11.341529369354248\t AVG Val Loss: 0.39516284068425495 \t AVG AUC: 0.5791442539944308\n",
      "19\t AVG Train Loss: 11.341065496206284\t AVG Val Loss: 0.3951553849379222 \t AVG AUC: 0.5795204931273593\n",
      "20\t AVG Train Loss: 11.341027736663818\t AVG Val Loss: 0.3951680517196655 \t AVG AUC: 0.5823765073314898\n",
      "21\t AVG Train Loss: 11.340525805950165\t AVG Val Loss: 0.39517072240511575 \t AVG AUC: 0.5832900252930909\n",
      "22\t AVG Train Loss: 11.340016275644302\t AVG Val Loss: 0.3951652789115906 \t AVG AUC: 0.5837093510543798\n",
      "23\t AVG Train Loss: 11.339773833751678\t AVG Val Loss: 0.39517890254656474 \t AVG AUC: 0.5852282955007758\n",
      "24\t AVG Train Loss: 11.339322030544281\t AVG Val Loss: 0.3951810892422994 \t AVG AUC: 0.5859754769802047\n",
      "25\t AVG Train Loss: 11.338823139667511\t AVG Val Loss: 0.39521574099858603 \t AVG AUC: 0.5879833639416976\n",
      "26\t AVG Train Loss: 11.337745577096939\t AVG Val Loss: 0.395218346118927 \t AVG AUC: 0.5899350564852348\n",
      "27\t AVG Train Loss: 11.336205035448074\t AVG Val Loss: 0.3952674659093221 \t AVG AUC: 0.5936266487273425\n",
      "28\t AVG Train Loss: 11.33439353108406\t AVG Val Loss: 0.39533028880755106 \t AVG AUC: 0.5968137010009911\n",
      "29\t AVG Train Loss: 11.331796675920486\t AVG Val Loss: 0.39536284605662025 \t AVG AUC: 0.6006941538773791\n",
      "30\t AVG Train Loss: 11.329037576913834\t AVG Val Loss: 0.3953116019566854 \t AVG AUC: 0.6043698346459341\n",
      "31\t AVG Train Loss: 11.32196819782257\t AVG Val Loss: 0.39539031942685443 \t AVG AUC: 0.6112710083833881\n",
      "32\t AVG Train Loss: 11.31558021903038\t AVG Val Loss: 0.3953648380438487 \t AVG AUC: 0.612944215380013\n",
      "33\t AVG Train Loss: 11.310262352228165\t AVG Val Loss: 0.39525359312693276 \t AVG AUC: 0.6125934733074857\n",
      "34\t AVG Train Loss: 11.304819285869598\t AVG Val Loss: 0.39524405598640444 \t AVG AUC: 0.6095479032597851\n",
      "35\t AVG Train Loss: 11.30271691083908\t AVG Val Loss: 0.3953825982411702 \t AVG AUC: 0.6144489501807077\n",
      "36\t AVG Train Loss: 11.292663812637329\t AVG Val Loss: 0.39591770807902016 \t AVG AUC: 0.6237149344981854\n",
      "37\t AVG Train Loss: 11.286384522914886\t AVG Val Loss: 0.39553742011388143 \t AVG AUC: 0.621270031967224\n",
      "38\t AVG Train Loss: 11.275900512933731\t AVG Val Loss: 0.39531328876813254 \t AVG AUC: 0.6168639365186248\n",
      "39\t AVG Train Loss: 11.273027002811432\t AVG Val Loss: 0.39519946058591204 \t AVG AUC: 0.6136540847242383\n"
     ]
    }
   ],
   "source": [
    "# # epsilon = range()\n",
    "# outputs, s_labels = agg_teacher(models, s_loader, 0.001) \n",
    "# print(outputs.shape, s_labels.shape)\n",
    "\n",
    "# student_model = HCModel(feat_dim=104)\n",
    "# optimizer = torch.optim.Adam(student_model.parameters(), lr=0.003)\n",
    "\n",
    "# train_losses, val_losses, val_aucs  = train_student(student_model, optimizer, s_loader, val_loader, STUDENT_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TBDdSkiF9Fc"
   },
   "source": [
    "## Interpret Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhF1Kza1GVAD"
   },
   "source": [
    "### Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "def plot_epoch(name, data, **kwargs):\n",
    "#     fp, tp, _ = roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(range(len(data)),data , label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(name.upper()+' AUC')\n",
    "#     plt.xlim([-0.5,len(labels)])\n",
    "    plt.ylim([0,0.01,1.05])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "jDdKxfiKnIV7",
    "outputId": "62c7195f-2720-48ca-994c-f221f50e6f65"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1667/321322379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# f, axarr = plt.subplots(1, 1, figsize=(10, 10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# axarr.plot(val_aucs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_aucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1667/1711346337.py\u001b[0m in \u001b[0;36mplot_epoch\u001b[0;34m(name, data, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' AUC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     plt.xlim([-0.5,len(labels)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd003/projects/aieng/public/pets_unified/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mylim\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1602\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1603\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd003/projects/aieng/public/pets_unified/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mset_ylim\u001b[0;34m(self, bottom, top, emit, auto, ymin, ymax)\u001b[0m\n\u001b[1;32m   3552\u001b[0m         \"\"\"\n\u001b[1;32m   3553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3554\u001b[0;31m             \u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mymin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbottom\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# f, axarr = plt.subplots(1, 1, figsize=(10, 10))\n",
    "# axarr.plot(val_aucs)\n",
    "plot_epoch(\"AUC\",val_aucs, color=colors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epoch(\"Val Losses\", val_losses, color=colors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-zJXvi9Gex2"
   },
   "source": [
    "### Privacy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GU9C2ewmnr9x",
    "outputId": "cfc1dd69-cc7d-46f4-c755-2df5f1e14bf1"
   },
   "outputs": [],
   "source": [
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=outputs, indices=s_labels, noise_eps=.2, delta=1e-5)\n",
    "print(f\"Data Dependent Analysis: {str(data_dep_eps)}\")\n",
    "print(f\"Data Independent Analysis: {str(data_ind_eps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWB56Xz18E5K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epsilons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1667/627226400.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepsilons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilons' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
