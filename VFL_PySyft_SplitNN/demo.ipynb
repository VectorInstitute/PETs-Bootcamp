{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG24P7DkIE93"
   },
   "source": [
    "# ***Introduction to Vertical Federated Learning*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsnR4ChbJD-Y"
   },
   "source": [
    "## **Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No2l7t-rpWjy"
   },
   "source": [
    "### Vertically Partitioned Data\n",
    "\n",
    "Often data exists in silos. This can be the case within an organization or among multiple organizations. In each case, the feature of a unique sample is distributed among multple datasets held by different parties. Exchanging data allows the parties to leverage a more complete feature set, thus enhancing the performance of the system. However, privacy restrictions often prevent the parties from doing so. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsRUCe5oIYvV"
   },
   "source": [
    "### Vertical Federated Learning\n",
    "\n",
    "Inspired by this limitation, [Vertical Federated Learning](https://blog.openmined.org/federated-learning-types/) (VFL) was introduced. VFL involves training a joint model on data that has features partitioned accross multiple datasets without explictly exchanging the features among parties.\n",
    "<p align=\"center\">\n",
    "<img width=\"433\" alt=\"Screen Shot 2021-09-28 at 5 41 20 PM\" src=\"https://user-images.githubusercontent.com/34798787/135169716-27e0e6a1-1d45-421d-882a-27241805b401.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7wYUKK0InzY"
   },
   "source": [
    "### Use Case\n",
    "\n",
    "A logical application of vertical federated learning is in the financial services industry, where several instituions hold credit data for the same individual. Although there is an inherent benenfit of exchanging data, instituions are restricted from doing so. To explore this scenario, we will be using the [Home Credit Default Dataset](https://www.kaggle.com/c/home-credit-default-risk/overview). This dataset contains features about credit applicants drawn from Home Credit's internal operations as well as data from the Credit Bureau. \n",
    "<p align=\"center\">\n",
    "<img width=\"759\" alt=\"Screen Shot 2021-09-28 at 3 42 48 PM\" src=\"https://user-images.githubusercontent.com/34798787/135170454-f937a987-0dcf-4b3a-a9f9-1bcbeac3bc9f.png\">\n",
    "</p>\n",
    "\n",
    "In order to apply VFL to this dataset, we have vertically partitioned into two datasets based on the origin of the features, Home Credit or the Credit Bureau. In total, there are ~300000 samples with corresponding labels.  The task at hand is to predicting future payment behavior of clients. \n",
    "<p align=\"center\">\n",
    "<img width=\"487\" alt=\"Screen Shot 2021-09-29 at 1 40 53 PM\" src=\"https://user-images.githubusercontent.com/34798787/135320845-db616df5-a9bd-4a26-a64d-fdf5c0ba3dcf.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0h6DtsZJpK4"
   },
   "source": [
    "### Tools\n",
    "In order to perform VFL, data and computation need to be distributed amongst the remote machine(s) of each party. However, deep learning frameworks like PyTorch and TensorFlow do not offer this functionality of the box. Fortunately [PySyft](https://blog.openmined.org/tag/pysyft/), a Python library for secure and private Deep Learning, extends both PyTorch and Keras to provide tools to perform VFL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maVYLc32Ixia"
   },
   "source": [
    "## **Demo Overview**\n",
    "In the following demo, VFL is used to train a joint model between Home Credit and the Credit Bureau to predict whether or not a credit applicants will default using PyTorch and PySyft. During this process, only abstract representations of features are shared thus eliminating the need to explictly exchange features. When coupled with differential privacy, VFL provides a robust solution to privacy preserving AI. \n",
    "\n",
    "The follwing demo contains three sections: \n",
    "1. **Data Preparation**\n",
    "    *  Define Vertical Dataloader \n",
    "    *  Find correspondences with Private Set Intersection\n",
    "\n",
    "2. **Model Preparation**\n",
    "    * Define Sub Model \n",
    "    * Define Split Neural Network\n",
    "\n",
    "3. **Training and Validation**\n",
    "    * Define train and validation loop\n",
    "    * Interpret results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNZVuP5Vkxf_"
   },
   "source": [
    "# ***Demo*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_NRtmOzVQcy"
   },
   "source": [
    "## **Environment Configuration and Package Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pw9JvgFs4l1_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from uuid import UUID, uuid4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import syft as sy\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data._utils.collate import Za1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1import syft as sy1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TP-CdD1C-evA"
   },
   "outputs": [],
   "source": [
    "from src.psi.util import Client, Server\n",
    "from src.utils import add_ids\n",
    "from src.utils.data_utils import id_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKMbs_mm06vp"
   },
   "source": [
    "## **Data Preparation**\n",
    "\n",
    "In the data preparation stage, the dataset for Home Credit and the Credit Bureau are initialized. Subsequently, the datasets are passed to a custom dataloader that computes the intersection of records between Home Credit and Credit Bureau using [Private Set Intersection](https://en.wikipedia.org/wiki/Private_set_intersection#:~:text=Private%20set%20intersection%20is%20a,the%20elements%20in%20the%20intersection.). In this way, only the unique identifiers of records are exchanged to compute the intersection. \n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"761\" alt=\"Screen Shot 2021-09-29 at 12 12 33 PM\" src=\"https://user-images.githubusercontent.com/34798787/135307783-8c251791-cf14-4cbd-827a-e1607127156d.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKd7jGUrLgXA"
   },
   "source": [
    "### Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEPZMWAc5tNz"
   },
   "outputs": [],
   "source": [
    "class VerticalDataset(Dataset):\n",
    "    \"\"\"Dataset for Vertical Federated Learning\"\"\"\n",
    "\n",
    "    def __init__(self, ids, data, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ids (Numpy Array) : Numpy Array with UUIDS\n",
    "            data (Numpy Array) : Numpy Array with Features\n",
    "            targets (Numpy Array) : Numpy Array with Labels. None if not available.\n",
    "        \"\"\"\n",
    "        self.ids = ids\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return record single record\"\"\"\n",
    "        feature = self.data[index].astype(np.float32)\n",
    "\n",
    "        if self.labels is None:\n",
    "            label = None\n",
    "        else:\n",
    "            label = int(self.labels[index]) if self.labels is not None else None\n",
    "\n",
    "        id = self.ids[index]\n",
    "\n",
    "        # Return a tuple of non-None elements\n",
    "        return (*filter(lambda x: x is not None, (feature, label, id)),)\n",
    "\n",
    "    def get_ids(self):\n",
    "        \"\"\"Return a list of the ids of this dataset.\"\"\"\n",
    "        return [str(id_) for id_ in self.ids]\n",
    "\n",
    "    def sort_by_ids(self):\n",
    "        \"\"\"\n",
    "        Sort the dataset by IDs in ascending order\n",
    "        \"\"\"\n",
    "        ids = self.get_ids()\n",
    "        sorted_idxs = np.argsort(ids)\n",
    "\n",
    "        self.data = self.data[sorted_idxs]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            self.labels = self.labels[sorted_idxs]\n",
    "\n",
    "        self.ids = self.ids[sorted_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKgLOOS--jCX"
   },
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "a5d_gwqX4uBx",
    "outputId": "3534cc20-a389-4a99-b121-7d69ee1d5a01"
   },
   "outputs": [],
   "source": [
    "# Load Home Credit Data\n",
    "data_dir = \"/ssd003/projects/pets/datasets/home_credit\"\n",
    "HC_DATA_PATH = f\"{data_dir}/home_credit_train.csv\"\n",
    "hc_df = pd.read_csv(HC_DATA_PATH)\n",
    "hc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "LcAHk5CN5T84",
    "outputId": "622168f1-f8be-49d9-9024-ff68588bc5ad"
   },
   "outputs": [],
   "source": [
    "# Load Credit Bureau Data\n",
    "CB_DATA_PATH = f\"{data_dir}/credit_bureau_train.csv\"\n",
    "cb_df = pd.read_csv(CB_DATA_PATH)\n",
    "cb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "ILpbPO20rNwh",
    "outputId": "b8431f32-fbf6-40a2-d2cf-c694923f9360"
   },
   "outputs": [],
   "source": [
    "# Carve out validation set\n",
    "assert cb_df.shape[0] == hc_df.shape[0]\n",
    "val_size = 20000\n",
    "val_ind = np.random.choice(range(cb_df.shape[0]), val_size, replace=False)\n",
    "hc_df_val = hc_df.iloc[val_ind]\n",
    "cb_df_val = cb_df.iloc[val_ind]\n",
    "hc_df.drop(val_ind, axis=0)\n",
    "cb_df.drop(val_ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zu_O4zUd-4zx"
   },
   "outputs": [],
   "source": [
    "# Get UID Column\n",
    "uuids = np.array([uuid4() for _ in range(len(hc_df))])\n",
    "uuids_val = np.array([uuid4() for _ in range(len(hc_df_val))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78HfKSy8YF6h"
   },
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlQCg9_tYDyP",
    "outputId": "874e707c-56d7-45ce-c0f9-69b453f82383"
   },
   "outputs": [],
   "source": [
    "## Home Credit Training Set\n",
    "print(f\"Training \\tHome Credit: {str(hc_df.shape)} Credit Bureau: {str(cb_df.shape)}\")\n",
    "print(\n",
    "    f\"Validation: \\tHome Credit: {str(hc_df_val.shape)} Credit Bureau: {str(cb_df_val.shape)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6bBPjaYylFZ"
   },
   "source": [
    "### Define Dataloader Classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzFk_cTqyp8j"
   },
   "outputs": [],
   "source": [
    "class SinglePartitionDataLoader(DataLoader):\n",
    "    \"\"\"DataLoader for a single vertically-partitioned dataset\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.collate_fn = id_collate_fn\n",
    "\n",
    "\n",
    "class VerticalDataLoader:\n",
    "    \"\"\"Dataloader which batches data from a complete\n",
    "    set of vertically-partitioned datasets\n",
    "    i.e. the images dataset AND the labels dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hc_data, cb_data, *args, **kwargs):\n",
    "\n",
    "        self.dataloader1 = SinglePartitionDataLoader(hc_data, *args, **kwargs)\n",
    "        self.dataloader2 = SinglePartitionDataLoader(cb_data, *args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Zip Dataloaders\n",
    "        \"\"\"\n",
    "        return zip(self.dataloader1, self.dataloader2)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return length of dataset\n",
    "        \"\"\"\n",
    "        return (len(self.dataloader1) + len(self.dataloader2)) // 2\n",
    "\n",
    "    def drop_non_intersecting(self, intersection):\n",
    "        \"\"\"Remove elements and ids in the datasets that are not in the intersection.\"\"\"\n",
    "        self.dataloader1.dataset.data = self.dataloader1.dataset.data[intersection]\n",
    "        self.dataloader1.dataset.ids = self.dataloader1.dataset.ids[intersection]\n",
    "\n",
    "        self.dataloader1.dataset.labels = self.dataloader1.dataset.labels[intersection]\n",
    "        self.dataloader2.dataset.ids = self.dataloader2.dataset.ids[intersection]\n",
    "\n",
    "    def sort_by_ids(self) -> None:\n",
    "        \"\"\"\n",
    "        Sort each dataset by ids\n",
    "        \"\"\"\n",
    "        self.dataloader1.dataset.sort_by_ids()\n",
    "        self.dataloader2.dataset.sort_by_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w26q6tXWPmL"
   },
   "source": [
    "## Initialize Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEJpOAlOScDM",
    "outputId": "1938ec31-d4bf-4fb5-91f7-f09ce0b3203c"
   },
   "outputs": [],
   "source": [
    "# Home Credit Dataset\n",
    "\n",
    "# Training\n",
    "hc_labels = np.array(hc_df.pop(\"target\"))\n",
    "hc_data = np.array(hc_df)\n",
    "print(\"train\", uuids.shape, hc_data.shape, hc_labels.shape)\n",
    "hc_dim = hc_data.shape[1]\n",
    "hc_dataset = VerticalDataset(ids=uuids, data=hc_data, labels=hc_labels)\n",
    "\n",
    "# Validation\n",
    "hc_labels_val = np.array(hc_df_val.pop(\"target\"))\n",
    "hc_data_val = np.array(hc_df_val)\n",
    "print(\"train\", uuids_val.shape, hc_data_val.shape, hc_labels_val.shape)\n",
    "hc_dataset_val = VerticalDataset(ids=uuids_val, data=hc_data_val, labels=hc_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27jdWoz3SR3x",
    "outputId": "51008665-cd8b-4e53-f810-f30fbefa4be5"
   },
   "outputs": [],
   "source": [
    "# Credit Bureau Dataset\n",
    "\n",
    "# Training\n",
    "cb_data = np.array(cb_df)\n",
    "print(cb_data.shape)\n",
    "cb_dim = cb_data.shape[1]\n",
    "cb_feat_dim = 4\n",
    "cb_dataset = VerticalDataset(ids=uuids, data=cb_data, labels=None)\n",
    "\n",
    "# Validation\n",
    "cb_dataset_val = np.array(cb_df_val)\n",
    "cb_dataset_val = VerticalDataset(ids=uuids_val, data=cb_dataset_val, labels=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QNYZA5Gzxcp"
   },
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMvmPtilV3nm"
   },
   "outputs": [],
   "source": [
    "## Initialize Train Dataloader\n",
    "dataloader = VerticalDataLoader(hc_dataset, cb_dataset, batch_size=2048)\n",
    "\n",
    "# Compute private set intersection\n",
    "client_items = dataloader.dataloader1.dataset.get_ids()\n",
    "server_items = dataloader.dataloader2.dataset.get_ids()\n",
    "\n",
    "client = Client(client_items)\n",
    "server = Server(server_items)\n",
    "\n",
    "setup, response = server.process_request(client.request, len(client_items))\n",
    "intersection = client.compute_intersection(setup, response)\n",
    "\n",
    "# Order data\n",
    "dataloader.drop_non_intersecting(intersection)\n",
    "dataloader.sort_by_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Db_HiiqpzCw8"
   },
   "outputs": [],
   "source": [
    "## Initialize Train Dataloader\n",
    "val_dataloader = VerticalDataLoader(hc_dataset_val, cb_dataset_val, batch_size=2048)\n",
    "\n",
    "# Compute private set intersection\n",
    "val_client_items = val_dataloader.dataloader1.dataset.get_ids()\n",
    "val_server_items = val_dataloader.dataloader2.dataset.get_ids()\n",
    "\n",
    "val_client = Client(val_client_items)\n",
    "val_server = Server(val_server_items)\n",
    "\n",
    "val_setup, val_response = val_server.process_request(\n",
    "    val_client.request, len(val_client_items)\n",
    ")\n",
    "val_intersection = val_client.compute_intersection(val_setup, val_response)\n",
    "\n",
    "# Order data\n",
    "val_dataloader.drop_non_intersecting(val_intersection)\n",
    "val_dataloader.sort_by_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYfxQcJAhoIF"
   },
   "source": [
    "## **Model Preparation**\n",
    "In the model preparation stage, both Home Credit and the Credit Bureau initialize a model. Subsequently, a [Split Neural Network](https://arxiv.org/pdf/1810.06060.pdf) (SplitNN) is defined with the Home Credit and Credit Bureau models as submodules. SplitNN orchestrates the flow of data and gradients within and accross submodules that are potentially on different machines. Data shared accross modules is in the form of representations to avoid explicitly exchanging features accross organizations. \n",
    "<p align=\"center\">\n",
    "<img width=\"400\" alt=\"Screen Shot 2021-09-29 at 1 10 17 PM\" src=\"https://user-images.githubusercontent.com/34798787/135316709-81bdb0ae-591c-4a67-8275-23f10c7b2610.png\">\n",
    "</p>\n",
    "\n",
    "In this demo, the SplitNN is defined as follows: \n",
    "<p align=\"center\">\n",
    "<img width=\"500\" alt=\"Screen Shot 2021-09-29 at 1 19 14 PM\" src=\"https://user-images.githubusercontent.com/34798787/135317891-49eb6157-ad14-4ec3-ae85-d5341e23b7c6.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zql8L1dk0yD"
   },
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpM-UicghngV"
   },
   "outputs": [],
   "source": [
    "class HCModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model for Credit Bureau\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cb_dim:\n",
    "        Dimensionality of Credit Bureau Data\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Credit Bureau Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hc_dim, cb_dim):\n",
    "        super(HCModel, self).__init__()\n",
    "        self.fused_input_dim = hc_dim + cb_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.fused_input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, hc_feat, cb_feat):\n",
    "        feat = torch.cat([hc_feat, cb_feat], dim=1)\n",
    "        pred = self.layers(feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfoqUZ9hqLj6"
   },
   "outputs": [],
   "source": [
    "class CBModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model for Credit Bureau\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cb_dim:\n",
    "        Dimensionality of Credit Bureau Data\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the Credit Bureau Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cb_dim):\n",
    "        super(CBModel, self).__init__()\n",
    "        self.cb_dim = cb_dim\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Linear(self.cb_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, cb_feat):\n",
    "        pred = self.layers(cb_feat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EYVzP575e0A"
   },
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    \"\"\"\n",
    "    A class representing SplitNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hc_model:\n",
    "        Home Credit Neural Network Module\n",
    "\n",
    "    cb_model:\n",
    "        Credit Bureau Neural Network Module\n",
    "\n",
    "    hc_opt:\n",
    "        Optimizer for the Home Credit Neural Network Module\n",
    "\n",
    "    cb_model:\n",
    "        Optimizer for the Credit Bureau Neural Network Module\n",
    "\n",
    "    data:\n",
    "        A list storing intermediate computations at each index\n",
    "\n",
    "    remote_tensors:\n",
    "        A list storing intermediate computations at each index (Computation from each model detached from global computation graph)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x):\n",
    "        Performs a forward pass through the SplitNN\n",
    "\n",
    "    backward():\n",
    "        Performs a backward pass through the SplitNN\n",
    "\n",
    "    zero_grads():\n",
    "        Zeros the gradients of all networks in SplitNN\n",
    "\n",
    "    step():\n",
    "        Updates the parameters of all networks in SplitNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hc_model, cb_model, hc_opt, cb_opt):\n",
    "        self.hc_model = hc_model\n",
    "        self.cb_model = cb_model\n",
    "        self.hc_opt = hc_opt\n",
    "        self.cb_opt = cb_opt\n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "\n",
    "    def forward(self, hc_x, cb_x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            Input Sample\n",
    "        \"\"\"\n",
    "\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "        # Forward pass through first model\n",
    "        data.append(self.cb_model(cb_x))\n",
    "\n",
    "        # if location of data is the same as location of the subsequent model\n",
    "        if data[-1].location == self.hc_model.location:\n",
    "            # store computation in remote tensor array\n",
    "            # Gradients will be only computed backward upto the point of detachment\n",
    "            remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "        else:\n",
    "            # else move data to location of subsequent model and store computation in remote tensor array\n",
    "            # Gradients will be only computed backward upto the point of detachment\n",
    "            remote_tensors.append(\n",
    "                data[-1].detach().move(self.hc_model.location).requires_grad_()\n",
    "            )\n",
    "\n",
    "        # Get and return final output of model\n",
    "        data.append(self.hc_model(hc_x, remote_tensors[-1]))\n",
    "\n",
    "        self.data = data\n",
    "        self.remote_tensors = remote_tensors\n",
    "        return data[-1]\n",
    "\n",
    "    def backward(self):\n",
    "        # if location of data is the same as detatched data\n",
    "        if self.remote_tensors[0].location == self.data[0].location:\n",
    "            # Store gradients from remote_tensor\n",
    "            grads = self.remote_tensors[0].grad.copy()\n",
    "        else:\n",
    "            # Move gradients to lovation of Store grad\n",
    "            grads = self.remote_tensors[0].grad.copy().move(self.data[0].location)\n",
    "\n",
    "        self.data[0].backward(grads)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.cb_opt.zero_grad()\n",
    "        self.hc_opt.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.cb_opt.step()\n",
    "        self.hc_opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjjtVRDrq8JB"
   },
   "source": [
    "### Initialize and Configure Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dip_9hZ2gaMN"
   },
   "outputs": [],
   "source": [
    "# Training globals\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Iniatialize Home Credit Model and Optimizer\n",
    "hc_model = HCModel(hc_dim, cb_feat_dim)\n",
    "hc_opt = torch.optim.Adam(hc_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# Iniatialize Credit Bureau Model and Optmizer\n",
    "cb_model = CBModel(cb_dim)\n",
    "cb_opt = torch.optim.Adam(cb_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# Define Split Neural Network\n",
    "splitNN = SplitNN(hc_model, cb_model, hc_opt, cb_opt)\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdLdg05Skk09"
   },
   "source": [
    "### Configure (Virtual) Remote Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fCSyYb2j63I"
   },
   "outputs": [],
   "source": [
    "# create some workers\n",
    "hc_worker = sy.VirtualWorker(hook, id=\"hc\")\n",
    "cb_worker = sy.VirtualWorker(hook, id=\"cb\")\n",
    "\n",
    "# Send Model Segments to model locations\n",
    "model_locations = [hc_worker, cb_worker]\n",
    "models = [hc_model, cb_model]\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqdJXcp9lhdI"
   },
   "source": [
    "## Training\n",
    "In the training phase, the SplitNN is trained jointly. During the forward and backward passes, we have to be ensure that the data and the model is on the same remote machine. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"479\" alt=\"Screen Shot 2021-09-29 at 1 27 48 PM\" src=\"https://user-images.githubusercontent.com/34798787/135318913-45308c81-5233-421c-8c03-5aa04060d2e0.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3lK6Q0gmfQX"
   },
   "source": [
    "### Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbwK_Yr_HBsb"
   },
   "outputs": [],
   "source": [
    "def train_step(dataloader, splitNN):\n",
    "    running_loss = 0\n",
    "    for (hc_data, labels, id1), (cb_data, id2) in dataloader:\n",
    "        # Send data and labels to machine model is on\n",
    "        labels = labels.float()\n",
    "        hc_data = hc_data.send(hc_model.location)\n",
    "        labels = labels.send(hc_model.location)\n",
    "        cb_data = cb_data.send(cb_model.location)\n",
    "\n",
    "        # Zero our grads\n",
    "        splitNN.zero_grads()\n",
    "\n",
    "        # Make a prediction\n",
    "        pred = splitNN.forward(hc_data, cb_data).squeeze()\n",
    "\n",
    "        # Figure out how much we missed by\n",
    "        loss = criterion(pred, labels)\n",
    "\n",
    "        # Backprop the loss on the end layer\n",
    "        loss.backward()\n",
    "        splitNN.backward()\n",
    "\n",
    "        # Change the weights\n",
    "        splitNN.step()\n",
    "\n",
    "        # Accumulate Loss\n",
    "        running_loss += loss.get()\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjB1LfvN0c62"
   },
   "outputs": [],
   "source": [
    "def val_step(val_dataloader, splitNN):\n",
    "    running_loss = 0\n",
    "    exs = 0\n",
    "    correct = 0\n",
    "    aucs = []\n",
    "    for (hc_data_val, labels_val, id1), (cb_data_val, id2) in val_dataloader:\n",
    "        # Send data and labels to machine model is on\n",
    "        labels_val = labels_val.float()\n",
    "        hc_data_val = hc_data_val.send(hc_model.location)\n",
    "        labels_val = labels_val.send(hc_model.location)\n",
    "        cb_data_val = cb_data_val.send(cb_model.location)\n",
    "\n",
    "        # Make a prediction\n",
    "        with torch.no_grad():\n",
    "            pred = splitNN.forward(hc_data_val, cb_data_val).squeeze()\n",
    "\n",
    "        # Calcualte Loss\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        loss = criterion(pred, labels_val)\n",
    "\n",
    "        # Calculate AUC\n",
    "        thresh_pred = (pred > 0.5).float()\n",
    "        thresh_pred = thresh_pred.get().int()\n",
    "        labels_val = labels_val.get().int()\n",
    "\n",
    "        # Fix Me: Undefined for batches with all-same labels...\n",
    "        auc = roc_auc_score(labels_val, pred.get().numpy())\n",
    "\n",
    "        # Calculate Accuracy Components\n",
    "        num_exs = hc_data_val.shape[0]\n",
    "        num_correct = torch.sum(thresh_pred == labels_val).item()\n",
    "\n",
    "        # Accumulate loss, accuracy and auc\n",
    "        exs += num_exs\n",
    "        correct += num_correct\n",
    "        running_loss += loss.get()\n",
    "        aucs.append(auc)\n",
    "\n",
    "    auc = np.mean(np.array(aucs))\n",
    "    accuracy = correct / exs\n",
    "\n",
    "    return auc, accuracy, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJPOWC3dHEyu"
   },
   "outputs": [],
   "source": [
    "metric_names = [\"Train Loss\", \"Validation Loss\", \"Accuracy\", \"AUC\"]\n",
    "metrics = {metric: [] for metric in metric_names}\n",
    "\n",
    "# Train Loop\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Train Step\n",
    "    train_loss = train_step(dataloader, splitNN)\n",
    "\n",
    "    # Train Step\n",
    "    auc, accuracy, val_loss = val_step(val_dataloader, splitNN)\n",
    "\n",
    "    # Log metrics\n",
    "    print(f\"Epoch: {i} \\t AUC: {auc}\")\n",
    "    metrics[\"Train Loss\"].append(train_loss.item())\n",
    "    metrics[\"Validation Loss\"].append(val_loss.item())\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"AUC\"].append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdpojwhzNFSr"
   },
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIBysym_NQDO"
   },
   "source": [
    "### Load Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbd2JxSENXdO"
   },
   "source": [
    "### Display Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtZJnc-Tg5bQ"
   },
   "outputs": [],
   "source": [
    "# Visualize Performance\n",
    "f, axarr = plt.subplots(1, 1, figsize=(10, 10))\n",
    "(vfl_line,) = axarr.plot(metrics[\"AUC\"], c=\"red\")\n",
    "vfl_line.set_label(\"VFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "78HfKSy8YF6h",
    "3QNYZA5Gzxcp"
   ],
   "machine_shape": "hm",
   "name": "pets_finance_demo_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
